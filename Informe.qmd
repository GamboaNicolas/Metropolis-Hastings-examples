---
title: "Metropolis-Hasting"
format: 
  pdf:
    fig-pos: "H"
lang: es
echo: FALSE
message: FALSE
warning: FALSE
tbl-cap-location: bottom
geometry:
  - top= 25mm
  - left= 20mm
  - right = 20mm
  - bottom = 25mm
  - heightrounded
header-includes:
  - \usepackage{ragged2e}
  - \usepackage{hyperref}

---


```{r Carga de librerias}
library(tidyverse)
library(mvtnorm)
library(patchwork)
library(tinytable)
library(pracma)
library(VGAM)
```

```{r Configuraciones predeterminadas}
knitr::opts_chunk$set(fig.align = "center", out.width = "70%")

set.seed("2126519")

theme_set(theme_bw())
```


# Introducción

Uno de los grandes problemas que existen en el mundo de la estadística es que hay distribuciones cuyas funciones de densidad son tan complejas que resultan difíciles de trabajar. Es por esto que muchas veces resulta de utilidad trabajar con muestras aleatorias, y a partir de estas responder determinadas preguntas. En la práctica, es común encontrarse con variables aleatorias con funciones de densidad cuyo muestreo directo no es simple.

Para estos casos existen distintas técnicas que nos permiten obtener muestras que, si bien no son tomadas de manera realmente independiente, se comportan de manera muy similar a como lo haría una muestra aleatoria independiente tomada de la función de densidad. 

Particularmente en el campo de la _Estadística Bayesiana_, esto resulta útil ya que permite obtener muestras de la distribución a posteriori, la cual se puede utilizar por ejemplo para obtener un intervalo de credibilidad del parámetro bajo estudio.

Un método sencillo de emplear y que reporta buenos resultados es el algoritmo de [_Metropolis-Hastings_](https://es.wikipedia.org/wiki/Algoritmo_de_Metropolis-Hastings). 

\newpage

# Metodología

En esta sección se presentan los algoritmo de metrópolis Hastingspara variables aleatorias unidimensionales y bidimensionales

(Agregar como funciona metropolis-hasting)

::: {.callout-note}

## Algoritmo Metrópolis-Hastingsunivariado

```{r algoritmo mh univariado doc, echo = T}
sample_mh <- function(n, d_objetivo, r_propuesta = NULL, d_propuesta = NULL, p_inicial = NULL){

  if (is.null(r_propuesta) | is.null(d_propuesta)) {
    r_propuesta <- function(media) rnorm(n = 1, media, sd = 1)
    d_propuesta <- function(x, media) dnorm(x = x,media, sd = 1)
  }
  
  stopifnot(n > 0)
  contador <- 0
  muestras <- numeric(n)
  
  muestras[1] <- p_inicial

  for(i in 2:n) {
    
    p_actual <- muestras[i-1]
    p_propuesta <- r_propuesta(p_actual)

    q_actual <- d_propuesta(p_actual, p_propuesta)
    q_nuevo <- d_propuesta(p_propuesta, p_actual)
    
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_propuesta)
    
    if (f_actual == 0 || q_nuevo == 0) {
      alfa <- 1
    } else {
      alfa <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    }

    muestras[i] <- sample(c(p_propuesta, p_actual),
    size = 1, prob = c(alfa, 1-alfa))
    
    if(muestras[i] != muestras[i-1]) {
      contador <- contador + 1
    }
  }
  return(list(cadena = data.frame(iteracion = 1:n, x = muestras), 
       tasa_aceptacion = contador / n))
}

```

:::

\newpage

::: {.callout-note}

## Algoritmo Metrópolis-Hastingsbivariado

```{r algoritmo m-h bivariado doc, echo = T}
sample_mh_mv <- function(n, d_objetivo, cov_propuesta = diag(2), p_inicial = numeric(2)) {
  
  if (length(p_inicial) != 2) {
    stop("El valor p_inicial debe ser bidimensional")
  }
  if ( n <= 0 || n %% 1 != 0) {
    stop("El tamaño de muestra n debe ser entero y mayor que 0")
  }
  if (any((dim(cov_propuesta) != c(2,2)))) {
    stop("La matriz de covariancia debe ser de 2x2")
  }
  
  contador <- 0
  
  r_propuesta <-  function(media) rmvnorm(n = 1,mean = media,sigma = cov_propuesta)
  d_propuesta <- function(x, media) dmvnorm(x = x,mean = media,sigma = cov_propuesta)
  
  muestras <- matrix(0,nrow = n,ncol = length(p_inicial))
  muestras[1, ] <- p_inicial
  
  for(i in 2:n) {
    p_actual <- muestras[i-1,]
    p_propuesta <- r_propuesta(p_actual)
    
    q_actual <- d_propuesta(p_actual, p_propuesta)
    q_nuevo <- d_propuesta(p_propuesta, p_actual)
    
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_propuesta)
    
    
    if (f_actual == 0 || q_nuevo == 0) {
      alfa <- 1
    } else {
      
      alfa <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    }
    
    aceptar <- rbinom(1,1,alfa)
    
    if (aceptar) {
      muestras[i,] <- p_propuesta
    }else{
      muestras[i,] <- p_actual
    }
    
    if(!any(muestras[i,] != muestras[i-1,])) {
      contador <- contador + 1
    }
  }
  salida <- data.frame(iteracion = 1:n, x = muestras)
  colnames(salida) <- c("iteracion", paste0("dim_",1:length(p_inicial)))
  
  
  return(list(muestra_mh = salida,
       probabilidad_aceptacion = contador / n)) # REVISAR -----------

}
```

:::

# Discusiones

## Distribución de Kumaraswamy

La distribución de Kumaraswamy es una distribución de probabilidad continua que se utiliza para modelar variables aleatorias con soporte en el intervalo $(0,1)$, cuya función de densidad es: $f(x|a,b) = abx^{a-1}(1-x^a)^{b-1}, \ con \ a,b>0$

```{r distribucion de kumaraswamy}
#| fig-cap: "Función de densidad de la distribución de Kumaraswamy"

grilla <- seq(0, 1, length.out = 502)[c(-1,-502)]
d_kuma <- function(x, a = 6, b = 2){
  
  if(a<0 | b<0) stop("Poneme a y b positivos no te cuesta nada")
  
  salida <- a*b*x^(a-1)*(1-x^a)^(b-1)
  
  salida[which(x <= 0 | x >= 1)] <- 0
  
  salida
}



tibble(
  x = rep(grilla,times = 5),
  Parámetros = rep(c("a = 0.5, b = 0.5",
                     "a = 1, b = 2",
                     "a = 3, b = 4",
                     "a = 6, b = 2",
                     "a = 5, b = 1"), each = 500),
  valores = c(
    d_kuma(grilla, 0.5, 0.5),
    d_kuma(grilla, 1, 2),
    d_kuma(grilla, 3, 4),
    d_kuma(grilla, 6, 2),
    d_kuma(grilla, 5,1)
  )
) |> 
  ggplot()+
  aes(x = x, y = valores,color = Parámetros)+
  geom_line(linewidth = 1.2) +
  labs(x = "X", y = expression(f ~ "("~x~" | a, b)"))

# tibble(
#   x = rep(grilla,times = 5),
#   parametros = rep(c("a = 0.5, b = 0.5",
#                      "a = 1, b = 2",
#                      "a = 3, b = 4",
#                      "a = 6, b = 2",
#                      "a = 5, b = 1"), each = 500),
#   valores = c(
#     dbeta(grilla, 0.5, 0.5),
#     dbeta(grilla, 1, 2),
#     dbeta(grilla, 3, 4),
#     dbeta(grilla, 6, 2),
#     dbeta(grilla, 5,1)
#   )
# ) |> 
#   ggplot()+
#   aes(x = x, y = valores,color = parametros)+
#   geom_line(linewidth = 1.2)
# 
```

Esta distribución puede ser utilizada en el ámbito de la estadística bayesiana a la hora de definir un prior para un parámetro con campo de variación en el intervalo (0, 1).

Por lo general la distribución elegida para estas situaciones suele ser la beta ya que presenta ventajas como ser una distribución conjugada de la binomial, lo cual puede facilitar mucho algunos cálculos. El problema es que la densidad de esta depende de la función gamma, la cual es una integral, y en algunas situaciones se puede complicar su cálculo.

La distribución de Kumaraswamy se comporta de manera muy similar a la beta, sin tener el problema de la dificultad del cálculo de la integral. 

## Metropolis-Hastingsen una dimensión

A continuación utilizaremos el algoritmo de Metropolis-Hastingspara generar 5000 muestras de la distribución de _Kumaraswamy_ con parámetros $a = 6$ y $b = 2$, utilizando como distribución propuesta una $Beta(\mu, \kappa)$, donde $\mu$ representa la media de la distribución y $\kappa$ el grado de la concentración de la distribución. Esto lo haremos para 3 valores distintos de $\kappa$.

```{r Algoritmo M-H univariado}
# sample_mh <- function(n,
#                       d_objetivo,
#                       r_propuesta = NULL,
#                       d_propuesta = NULL,
#                       p_inicial = NULL){
# 
#   if (is.null(r_propuesta) | is.null(d_propuesta)) {
#     r_propuesta <- function(media) rnorm(n = 1, media, sd = 1)
#     d_propuesta <- function(x, media) dnorm(x = x,media, sd = 1)
#   }
# 
#   stopifnot(n > 0)
#   contador <- 0
#   muestras <- numeric(n)
# 
#   # if (is.null(p_inicial)) {
#   #   for(i in 1:1000){
#   #     p_inicial <- runif(1,-100,100)
#   #     if (d_objetivo(p_inicial) != 0) {
#   #       break
#   #     }
#   #   }
#   #   if(i == 1000) stop("Se recomienda introducir un valor para ´p_inicial´")
#   # }
# 
#   muestras[1] <- p_inicial
# 
#   for(i in 2:n) {
# 
#     p_actual <- muestras[i-1]
#     p_propuesta <- r_propuesta(p_actual)
# 
#     q_actual <- d_propuesta(p_actual, p_propuesta)
#     q_nuevo <- d_propuesta(p_propuesta, p_actual)
# 
#     f_actual <- d_objetivo(p_actual)
#     f_nuevo <- d_objetivo(p_propuesta)
# 
#     if (f_actual == 0 || q_nuevo == 0) {
#       alfa <- 1
#     } else {
# 
#       alfa <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
#     }
# 
# 
#     muestras[i] <- sample(c(p_propuesta, p_actual),
#     size = 1, prob = c(alfa, 1-alfa))
# 
#     if(muestras[i] != muestras[i-1]) {
#       contador <- contador + 1
#     }
#   }
#   return(list(cadena = data.frame(iteracion = 1:n, x = muestras),
#        tasa_aceptacion = contador / n))
# }
# 
# 
# # d_obj <- function(x){
# #   # dnorm(x = x,mean = 90,sd = 8)
# #   return(dbeta(x = x,2,2))
# # }
# # d_obj(-50)
# # muestra <- sample_mh(100000, d_objetivo = d_obj)
# #
# # muestra |>
# #   ggplot()+
# #   aes(x = x)+
# #   geom_histogram()
```

```{r comparacion de las cadenas}
source("Funciones.R")

get_beta_param <- function(media, kappa) {
  alpha <- media*kappa
  beta <- (1-media)*kappa
  
  return(list(alpha = alpha, beta = beta))
}
```


```{r cadenas markov, include=F}
valores_kappa <- c(1, 2, 5)

graficos <- list(Kappa1 = NULL, Kappa2 = NULL, Kappa5 = NULL)

df_cadenas <- list(data_cadena_1 = NULL, data_cadena_2 = NULL, data_cadena_3 = NULL)

for(i in 1:3) {
  r_prop <- function(x) {
  kappa <- valores_kappa[i]
  pars <- get_beta_param(x, kappa)
  rbeta(1, shape1 = pars$alpha, shape2 = pars$beta)
}
d_prop <- function(x, mean) {
  kappa <- valores_kappa[i]
  pars <- get_beta_param(mean, kappa)
  dbeta(x, shape1 = pars$alpha, shape2 = pars$beta)
}

muestra <- sample_mh(n = 5000, d_objetivo = d_kuma, r_propuesta = r_prop, d_propuesta = d_prop, p_inicial = rbeta(1,2,2))

ap1 <- plot_autocor(muestra = muestra$cadena)

tp1 <- plot_trace(muestra = muestra$cadena)

hp1 <- plot_hist(muestra = muestra$cadena, d_objetivo = d_kuma)

graficos[[i]] <- (tp1 / (hp1 + ap1))

df_cadenas[[i]] <- muestra
}

neff1 <- n_eff(df_cadenas$data_cadena_1$cadena[2])
neff2 <- n_eff(df_cadenas$data_cadena_2$cadena[2])
neff3 <- n_eff(df_cadenas$data_cadena_3$cadena[2])

```


```{r cadena 1}
#| fig-cap: "Muestreo por Metropolis-Hastings con $\\kappa$ = 1"

graficos$Kappa1
```


```{r cadena 2}
#| fig-cap: "Muestreo por Metropolis-Hastings con $\\kappa$ = 2"

graficos$Kappa2
```


```{r cadena 3}
#| fig-cap: "Muestreo por Metropolis-Hastings con $\\kappa$ = 5"

graficos$Kappa5
```

Teniendo en cuenta que estas muestras son dependientes resulta de interés conocer a cuantas muestras independientes equivalen, dado que cuanto más se comporta una cadena de Markov dependiente como una muestra independiente, menor es el error en la aproximación posterior resultante. Para ello se calcula el número efectivo de muestras $(n_{eff})$ para cada cadena.

```{r tabla nro efectivo de muestra}
cadenas_text <- c("Cadena 1", "Cadena 2", "Cadena 3")

nro_ef_muestras <- tibble(
  Cadena = cadenas_text,
  kappa = rep(c(1,2,5)),
  n_eff = floor(c(neff1,neff2,neff3))
) |> `colnames<-`(c("Cadena", "$\\kappa$", "$n_{eff}$"))
```


```{r tabla nro efectivo de muestra 2}
#| tbl-cap: "Numero efectivo de muestras para cadenas de 5000 muestras y distintos valores de $\\kappa$"

nro_ef_muestras |> 
  tt() 
```

Con esto concluimos que el valor de $\kappa$ que arroja una muestra relacionada que equivale a una independiente de mayor tamaño es 5. Sin embargo esta cantidad de muestras independientes es muy pobre teniendo en cuenta que el tamaño de la cadena es de 5000 muestras.


```{r funcion x}

# Creación de la tabla

# Cadena 1
media_cadena_1 <- mean(df_cadenas$data_cadena_1$cadena$x) |> round(3)
percentiles_1 <- quantile(df_cadenas$data_cadena_1$cadena$x, c(0.05,0.95)) |> round(3)

# Cadena 2
media_cadena_2 <- mean(df_cadenas$data_cadena_2$cadena$x) |> round(3)
percentiles_2 <- quantile(df_cadenas$data_cadena_2$cadena$x, c(0.05,0.95)) |> round(3)

# Cadena 3
media_cadena_3 <- mean(df_cadenas$data_cadena_3$cadena$x) |> round(3)
percentiles_3 <- quantile(df_cadenas$data_cadena_3$cadena$x, c(0.05,0.95)) |> round(3)

# Media y cuantiles verdaderos

g <- function(x) x*d_kuma(x, 6, 2)
media_kumar <- integrate(g, 0, 1)$value |> round(digits = 3)

kumar_05 <- qkumar(p = 0.05, shape1 = 6, shape2 = 2) |> round(digits = 3)
kumar_95 <- qkumar(p = 0.95, shape1 = 6, shape2 = 2) |> round(digits = 3)

logit <- function(x) log(x/(1-x))

# Cadena 1
media_cadena_lg_1 <- mean(logit(df_cadenas$data_cadena_1$cadena$x)) |> round(3)
percentiles_lg_1 <- quantile(logit(df_cadenas$data_cadena_1$cadena$x), c(0.05,0.95)) |> round(3)

# Cadena 2
media_cadena_lg_2 <- mean(logit(df_cadenas$data_cadena_2$cadena$x)) |> round(3)
percentiles_lg_2 <- quantile(logit(df_cadenas$data_cadena_2$cadena$x), c(0.05,0.95)) |> round(3)

# Cadena 3
media_cadena_lg_3 <- mean(logit(df_cadenas$data_cadena_3$cadena$x)) |> round(3)
percentiles_lg_3 <- quantile(logit(df_cadenas$data_cadena_3$cadena$x), c(0.05,0.95)) |> round(3)

# Media y cuantiles verdaderos
g_lg <- function(x) logit(x)*(d_kuma(x, 6, 2))

media_kumar_lg <- integrate(g_lg, 0, 1)$value |> round(digits = 3)
kumar_05_lg <- logit(qkumar(p = 0.05, shape1 = 6, shape2 = 2)) |> round(digits = 3)
kumar_95_lg <- logit(qkumar(p = 0.95, shape1 = 6, shape2 = 2)) |> round(digits = 3)

```

\newpage


Resulta de interés ver como afecta la elección del parámetro $\kappa$ al utilizar Metropolis-Hastingspara obtener muestras de la distribución de Kumaraswamy usando una distribución $Beta$ como propuesta. Es por esto que se decide obtener la media y ciertos cuantiles sobre las muestras obtenidas y sobre la función Logit de estas.

```{r tablita fachera}
filas <- rep(c("$X$", "$Logit(X)$"), each = 3)

valores <- tibble(
  var = filas,
  kappa = rep(c("1","2","5"), times = 2), 
  esp_est = c(media_cadena_1, media_cadena_2, media_cadena_3, media_cadena_lg_1, media_cadena_lg_2, media_cadena_lg_3),
  quantil_0.05 = c(percentiles_1[1], percentiles_2[1], percentiles_3[1], percentiles_lg_1[1], percentiles_lg_2[1], percentiles_lg_3[1]),
  quantil_0.95 = c(percentiles_1[2], percentiles_2[2], percentiles_3[2], percentiles_lg_1[2], percentiles_lg_2[2], percentiles_lg_3[2])
) |> 
  `colnames<-`(c("$f(x)$" ,"$\\kappa$", "$\\hat{E(x)}$", "$\\hat{q_{0.05}}$", "$\\hat{q_{0.95}}$"))
```


```{r tablita fachera 2}
#| tbl-cap: "Media y quantiles estimadas para las funciones $X$ y $Logit(X)$"

valores |> 
  tt() |> 
  style_tt(i = c(1,4), j = 1, rowspan = 3, alignv = "t")
```


Las estadísticas reales de la distribución de Kumaraswamy son:

```{r valores reales kuma}
filas <- c("$X$", "$Logit(X)$")

valores_reales <- tibble(
  var = filas,
  esp_est = c(media_kumar,media_kumar_lg),
  quantil_0.05 = c(kumar_05,kumar_05_lg),
  quantil_0.95 = c(kumar_95,kumar_95_lg)
) |> 
  `colnames<-`(c("$f(x)$" ,"$E(x)$", "$q_{0.05}$", "$q_{0.95}$"))
```


```{r valores reales kuma 2}
#| tbl-cap: "Media y quantiles para las funciones $X$ y $Logit(X)$"

valores_reales |> 
  tt()
```

Si bien las diferencias en dispersión de las muestras no son muy perceptibles a simple vista, gracias a la funcióm logit podemos percibirlas con mayor facilidad. Es así que se puede concluir que usando una distribución $Beta$ con un parámetro $\kappa = 2$ de concentración, es que se obtienen las estimaciones más cercanas a la distribución de Kumaraswamy con parámetros $a = 6$ y $b = 2$. 


## Metropolis-Hastings en dos dimensiones


El algoritmo de Metropolis-Hastings se aprecia más cuando se obtienen muestras de distribuciones en más de una dimensión, incluso cuando no se conoce la constante de normalización, sin embargo tiene limitaciones para cierta funciones, las cuales se verán a continuación.



```{r}
sample_mh_mv <- function(n, d_objetivo, cov_propuesta = diag(2), p_inicial = numeric(2)) {
  
  if (length(p_inicial) != 2) {
    stop("El valor p_inicial debe ser bidimensional")
  }
  if ( n <= 0 || n %% 1 != 0) {
    stop("El tamaño de muestra n debe ser entero y mayor que 0")
  }
  if (any((dim(cov_propuesta) != c(2,2)))) {
    stop("La matriz de covariancia debe ser de 2x2")
  }
  
  contador <- 0
  
  r_propuesta <-  function(media) rmvnorm(n = 1,mean = media,sigma = cov_propuesta)
  d_propuesta <- function(x, media) dmvnorm(x = x,mean = media,sigma = cov_propuesta)
  
  muestras <- matrix(0,nrow = n,ncol = length(p_inicial))
  muestras[1, ] <- p_inicial
  
  for(i in 2:n) {
    p_actual <- muestras[i-1,]
    p_propuesta <- r_propuesta(p_actual)
    
    q_actual <- d_propuesta(p_actual, p_propuesta)
    q_nuevo <- d_propuesta(p_propuesta, p_actual)
    
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_propuesta)
    
    
    if (f_actual == 0 || q_nuevo == 0) {
      alfa <- 1
    } else {
      
      alfa <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    }
    
    aceptar <- rbinom(1,1,alfa)
    
    if (aceptar) {
      muestras[i,] <- p_propuesta
    }else{
      muestras[i,] <- p_actual
    }
    
    if(!any(muestras[i,] != muestras[i-1,])) {
      contador <- contador + 1
    }
  }
  salida <- data.frame(iteracion = 1:n, x = muestras)
  colnames(salida) <- c("iteracion", paste0("dim_",1:length(p_inicial)))
  
  
  return(list(muestra_mh = salida,
       probabilidad_aceptacion = contador / n)) # REVISAR -----------

}
```

Se quiere obtener muestras de una distribución Normal bivariada con media , para ello se aplica el algoritmo de Metropolis-Hastings y se proponen 

Para testear el algoritmo de Metropolis-Hastings bivariado creado, se decide obtener muestras de una distribución Normal bivariada con vector de media $\mu^*$ y matriz de covarianza $\Sigma^*$ con el algoritmo ya mencionado.

Donde $\mu^* = \begin{bmatrix} 0.4 \\ 0.75 \end{bmatrix}$ y $\Sigma^* = \begin{bmatrix} 1.35 & 0.4 \\ 0.4 & 2.4 \end{bmatrix}$.

Se plantean 3 matrices de covarianzas distintas como propuestas para observar con cual de estas se obtienen muestras mas aproximadas a la verdadera distribución:

1. $\Sigma_1 = \begin{bmatrix} 1 & 0 \\ 0 & 2 \end{bmatrix}$ Se puede ver en @fig-cov1 y @fig-trace-cov1

2. $\Sigma_2 = \begin{bmatrix} 0.25 & 0 \\ 0 & 0.25 \end{bmatrix}$ Se puede ver en @fig-cov2 y @fig-trace-cov2

3. $\Sigma_3 = \begin{bmatrix} 1 & 0.3 \\ 0.3 & 1 \end{bmatrix}$ Se puede ver en @fig-cov3 y @fig-trace-cov3

```{r}
mu <- c(.4,.75) 
sigma <- matrix(c(1.35,.4,.4,2.4),2)
d_obj <- function(x) dmvnorm(x,mean = mu,sigma = sigma)

cov_prop_1 <- diag(c(1,2))
cov_prop_2 <- diag(c(0.25,0.25))
cov_prop_3 <- matrix(c(1,0.3,0.3,1), ncol = 2)

cadena1 <- sample_mh_mv(n = 5000,d_objetivo = d_obj, cov_propuesta = cov_prop_1, p_inicial = c(0,0))
cadena2 <- sample_mh_mv(n = 5000,d_objetivo = d_obj, cov_propuesta = cov_prop_2, p_inicial = c(0,0))
cadena3 <- sample_mh_mv(n = 5000,d_objetivo = d_obj, cov_propuesta = cov_prop_3, p_inicial = c(0,0))

```



```{r distribucion normal bivariada 1}
#| fig-cap: "Distribución de las muestras obtenidas por Metropolis-Hastings de una $\\mathcal{N}_2(\\mu^*, \ \\Sigma^*)$ con $\\Sigma_1$ como propuesta"
#| label: fig-cov1

plot_hotmap(muestra = cadena1$muestra_mh, d_objetivo = d_obj, puntos = F)
```

```{r traceplot normal bivariada 1}
#| fig-cap: "Cadena de Markov de las muestras obtenidas por Metropolis-Hastings de una $\\mathcal{N}_2(\\mu^*, \ \\Sigma^*)$ con $\\Sigma_1$ como propuesta"
#| label: fig-trace-cov1

plot_trace(muestra = cadena1$muestra_mh)

```

```{r distribucion normal bivariada 2}
#| fig-cap: "Distribución de las muestras obtenidas por Metropolis-Hastings de una $\\mathcal{N}_2(\\mu^*, \ \\Sigma^*)$ con $\\Sigma_2$ como propuesta"
#| label: fig-cov2

plot_hotmap(muestra = cadena2$muestra_mh, d_objetivo = d_obj, puntos = F)
```


```{r traceplot normal bivariada 2}
#| fig-cap: "Cadena de Markov de las muestras obtenidas por Metropolis-Hastings de una $\\mathcal{N}_2(\\mu^*, \ \\Sigma^*)$ con $\\Sigma_2$ como propuesta"
#| label: fig-trace-cov2

plot_trace(muestra = cadena2$muestra_mh)
```


```{r distribucion normal bivariada 3}
#| fig-cap: "Distribución de las muestras obtenidas por Metropolis-Hastings de una $\\mathcal{N}_2(\\mu^*, \ \\Sigma^*)$ con $\\Sigma_3$ como propuesta"
#| label: fig-cov3

plot_hotmap(muestra = cadena3$muestra_mh, d_objetivo = d_obj, puntos = F)
```


```{r traceplot normal bivariada 3}
#| fig-cap: "Cadena de Markov de las muestras obtenidas por Metropolis-Hastings de una $\\mathcal{N}_2(\\mu^*, \ \\Sigma^*)$ con $\\Sigma_3$ como propuesta"
#| label: fig-trace-cov3

plot_trace(muestra = cadena3$muestra_mh)
```

(Chamuyar acerca de la mejor matriz de covarianzas... utilizando las estadisticas de bondad de la muestra)

\newpage

Para ver la bondad de la muestra generada por Metropolis-Hastings, se calculan ciertas probabilidades y luego se comparan con otros métodos de cálculo de probabilidades, como "Función de distribución" y "Método de MonteCarlo".

```{r calculo de las probabilidades normal bivariada estimadas}
# i
probabilidad_p8_1 <- mean(cadena1$muestra_mh$dim_1 > 1 & cadena1$muestra_mh$dim_2 < 0) |> round(4)

# ii
probabilidad_p8_2 <- mean(cadena1$muestra_mh$dim_1 > 1 & cadena1$muestra_mh$dim_2 > 2) |> round(4)

# iii
probabilidad_p8_3 <- mean(cadena1$muestra_mh$dim_1 > 0.4 & cadena1$muestra_mh$dim_2 > 0.75) |> round(4)

# i funcion de distribucion
prob_1 <- as.numeric(pmvnorm(lower = c(1, -Inf), upper = c(Inf, 0), mean = mu, sigma = sigma)) |> round(4)

# ii funcion de distribucion
prob_2 <- as.numeric(pmvnorm(lower = c(1, 2), upper = c(Inf, Inf), mean = mu, sigma = sigma)) |> round(4)

# iii funcion de distribucion
prob_3 <- as.numeric(pmvnorm(lower = c(0.4, 0.75), upper = c(Inf, Inf), mean = mu, sigma = sigma)) |> round(4)

muestras_normal_bivariada <- rmvnorm(5000, mean = c(0.4, 0.75), sigma = matrix(c(1.35,.4,.4,2.4),2)) |> round(4)

# i MCMC
prob_1_mcmc <- mean(muestras_normal_bivariada[,1] > 1 & muestras_normal_bivariada[,2] < 0) |> round(4)

# ii MCMC
prob_2_mcmc <- mean(muestras_normal_bivariada[,1] > 1 & muestras_normal_bivariada[,2] > 2) |> round(4)

# iii MCMC
prob_3_mcmc <- mean(muestras_normal_bivariada[,1] > 0.4 & muestras_normal_bivariada[,2] > 0.75) |> round(4)

```

```{r tabla para probabilidad p8}
filas <- c("M-H", "Función de \ndistribución", "MC")

probabilidades_nb <- tibble(
  var = filas,
  p1 = c(probabilidad_p8_1, prob_1, prob_1_mcmc),
  p2 = c(probabilidad_p8_2, prob_2, prob_2_mcmc),
  p3 = c(probabilidad_p8_3, prob_3, prob_3_mcmc)
) |> `colnames<-`(c("Método","$P(X_1 > 1, X_2 < 0)$", "$P(X_1 > 1, X_2 > 2)$", "$P(X_1 > 0.4, X_2 > 0.75)$"))
```

```{r tabla probabilidades 1}
#| tbl-cap: "Cálculo de probablidades con distintos métodos"

probabilidades_nb |> 
  tt() |> 
  style_tt(
    j = 1:4,
    line = "tblr",
    line_width = 0.1,
    line_color = "black")
```

Podemos concluir que las estimaciones de las probabilidades calculadas son bastante acertadas, debido a su similitud con los otros métodos.

## Función de Rosenbrock

Es una función matemática utilizada frecuentemente como prueba de algoritmos de optimización numérica. En el campo de la Estadística Bayesiana, es muy conocida dado que la densidad del posterior toma una forma que definitivamente se asemeja a la _banana de Rosenbrock_. (Redactar??)

La densidad de ésta función viene dada por: $p^*(x_1,x_2|a,b) = exp\{-[(a-x_1)^2+b(x_2-x_1^2)^2]\}$

Para poner a prueba el algoritmo de Metropolis-Hastings bivariado en situaciones complejas, se decide obtener 5000 muestras de la función anteriormente mencionada, con parámetros $a = 0.5$ y $b = 5$. Se utilizan 3 distintas matrices de covarianzas propuestas, las cuáles son:

1. $\Sigma_1 = \begin{bmatrix} 0.06 & 0 \\ 0 & 0.07 \end{bmatrix}$ Se puede ver su densidad en @fig-banana1 y el recorrido de la cadena en @fig-bananatrace1. Con esta matriz la probabilidad de aceptación es (insertar valor)

2. $\Sigma_2 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ Se puede ver su densidad en @fig-banana2 y el recorrido de la cadena en @fig-bananatrace2. Con esta matriz la probabilidad de aceptación es (insertar valor)

3. $\Sigma_3 = \begin{bmatrix} 0.5 & 0 \\ 0 & 0.5 \end{bmatrix}$ Se puede ver su densidad en @fig-banana3 y el recorrido de la cadena en @fig-bananatrace3. Con esta matriz la probabilidad de aceptación es (insertar valor)

```{r banana de rosenbrock 1}
d_obj_p <- function(x, a = 0.5, b = 5) exp(-((a-x[1])^2 + b *(x[2]-x[1]^2)^2))

cov_prop_p <- diag(c(0.06, 0.07))

cadena_p <- sample_mh_mv(n = 5000, d_objetivo = d_obj_p, cov_propuesta = cov_prop_p, p_inicial = c(0,0))

# cadena_p$probabilidad_aceptacion
# 
# acf(cadena_p$muestra_mh[-1], plot = F) # Decorar
```


```{r hotmap de rosenbrock 1}
#| fig-cap: "Distribución de las muestras obtenidas por Metropolis-Hastings de $p^*$ con $\\Sigma_1$ como propuesta"
#| label: fig-banana1

plot_hotmap(cadena_p$muestra_mh[-1], d_objetivo = d_obj_p, puntos = F)
```

```{r traceplot banana de rosenbrock 1}
#| fig-cap: "Cadena de Markov de las muestras obtenidas por Metropolis-Hastings de $p^*$ con $\\Sigma_1$ como propuesta"
#| label: fig-bananatrace1

plot_trace(cadena_p$muestra_mh)
```



```{r banana de rosenbrock 2}
cov_prop_p <- diag(c(1, 1))

cadena_p <- sample_mh_mv(n = 5000, d_objetivo = d_obj_p, cov_propuesta = cov_prop_p, p_inicial = c(0,0))

# cadena_p$probabilidad_aceptacion
# 
# acf(cadena_p$muestra_mh[-1], plot = F) # Decorar
```

```{r hotmap de rosenbrock 2}
#| fig-cap: "Distribución de las muestras obtenidas por Metropolis-Hastings de $p^*$ con $\\Sigma_2$ como propuesta"
#| label: fig-banana2

plot_hotmap(cadena_p$muestra_mh[-1], d_objetivo = d_obj_p, puntos = F)
```

```{r traceplot banana de rosenbrock 2}
#| fig-cap: "Cadena de Markov de las muestras obtenidas por Metropolis-Hastings de $p^*$ con $\\Sigma_2$ como propuesta"
#| label: fig-bananatrace2

plot_trace(cadena_p$muestra_mh)
```

```{r banana de rosenbrock 3}
cov_prop_p <- diag(c(0.5, 0.5))

cadena_p <- sample_mh_mv(n = 5000, d_objetivo = d_obj_p, cov_propuesta = cov_prop_p, p_inicial = c(0,0))

# cadena_p$probabilidad_aceptacion
# 
# 
# acf(cadena_p$muestra_mh[-1], plot = F) # Decorar
```

```{r banana de rosenbrock 3}
#| fig-cap: "Distribución de las muestras obtenidas por Metropolis-Hastings de $p^*$ con $\\Sigma_3$ como propuesta"
#| label: fig-banana3

plot_hotmap(cadena_p$muestra_mh[-1], d_objetivo = d_obj_p, puntos = F)
```

```{r banana de rosenbrock 3}
#| fig-cap: "Cadena de Markov de las muestras obtenidas por Metropolis-Hastings de $p^*$ con $\\Sigma_3$ como propuesta"
#| label: fig-bananatrace3

plot_trace(cadena_p$muestra_mh)
```

(Hablar acerca de la probabilidad de aceptación y de con cuál se obtienen las mejores muestras).

\newpage

Para ver la bondad de la muestra generada por Metropolis-Hastings, se calculan ciertas probabilidades y luego se comparan con otros métodos de cálculo de probabilidades, como "Función de distribución" y "Método de MonteCarlo".

Para ver que tan bien funciona Metropolis-Hastings en esta situación compleja, interesa calcular ciertas probabilidades y compararlas con el método de "Integración Manual".

```{r probabilidades p10 1}
# i
pr_banana1 <- mean((cadena_p$muestra_mh$dim_1 > 0 & cadena_p$muestra_mh$dim_1 < 1) & 
       (cadena_p$muestra_mh$dim_2 > 0 & cadena_p$muestra_mh$dim_2 < 1)) |> round(4)

# ii
pr_banana2 <- mean((cadena_p$muestra_mh$dim_1 > -1 & cadena_p$muestra_mh$dim_1 < 0) & 
       (cadena_p$muestra_mh$dim_2 > 0 & cadena_p$muestra_mh$dim_2 < 1)) |> round(4)

# iii
pr_banana3 <- mean((cadena_p$muestra_mh$dim_1 > 1 & cadena_p$muestra_mh$dim_1 < 2) & 
       (cadena_p$muestra_mh$dim_2 > 2 & cadena_p$muestra_mh$dim_2 < 3)) |> round(4)
```

```{r probabilidades p10 2}
d_integral <- function(x, y) exp(-((0.5-x)^2 + 5 *(y-x^2)^2))

pr_valor_real1 <- integral2(d_integral, xmin = 0, xmax = 1, ymin = 0, ymax = 1)$Q |> round(4)

pr_valor_real2 <- integral2(d_integral, xmin = -1, xmax = 0, ymin = 0, ymax = 1)$Q |> round(4)

pr_valor_real3 <- integral2(d_integral, xmin = 1, xmax = 2, ymin = 2, ymax = 3)$Q |> round(4)

```

```{r tabla para probabilidad p10 1}
filas <- c("M-H", "Integración manual")

probabilidades_banana <- tibble(
  var = filas,
  p1 = c(pr_banana1, pr_valor_real1),
  p2 = c(pr_banana2, pr_valor_real2),
  p3 = c(pr_banana3, pr_valor_real3)
) |> `colnames<-`(c("Método","$P(0 < X_1 < 1, 0 < X_2 < 1)$", "$P(-1 < X_1 < 0, 0 < X_2 < 1)$", "$P(1 < X_1 < 2, 2 < X_2 < 3)$"))
```

```{r tabla probabilidades banana 1}
#| tbl-cap: "Cálculo de probablidades con distintos métodos"

probabilidades_banana |> 
  tt() |> 
  style_tt(
    j = 1:4,
    line = "tblr",
    line_width = 0.1,
    line_color = "black")
```

Se puede observar que el algoritmo en esta situación falla bastante en la generación de las muestras, dado que la estimación de las probabilidades son muy diferentes a las calculadas por un método casi exacto.

\newpage

# Conclusiones

(Redactar las conclusiones en un futuro no muy lejano ...)

# Preguntas y propuestas

Comparar las probabilidades tmb por monte carlo
Preguntar por la probabilidad de salto en mh_multivariado
Como es el numerador del NEFF (Se calcula con una o varias cadenas?)
Como validar la bondad del metodo en el punto 7, que estadisticas podemos usar?, algun test?
Punto 8, calculo de probabilidades con una sola cadena?
Punto 9, Calculo de la probabilidad de aceptacion y funcion de autocorrelacion bivariada.
Punto 10, se puede considerar integracion manual si lo realizamos con un paquete??