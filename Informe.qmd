---
format: 
  pdf:
    fig-pos: "H"
    tbl-cap-location: bottom
lang: es
echo: FALSE
message: FALSE
warning: FALSE
geometry:
  - top= 25mm
  - left= 20mm
  - right = 20mm
  - bottom = 25mm
  - heightrounded
header-includes:
  - \usepackage{ragged2e}
  - \usepackage{hyperref}
  - \usepackage{float}
  - \floatplacement{table}{H}
---


```{r Carga de librerias y funciones}
library(tidyverse)
library(mvtnorm)
library(patchwork)
library(tinytable)
library(pracma)
library(VGAM)

# Funciones a utilizar
source("Funciones.R")
```

```{r Configuraciones predeterminadas}
knitr::opts_chunk$set(fig.align = "center", out.width = "70%")

set.seed("2126519")

theme_set(theme_bw())
```

::: {.center data-latex=""}

\vspace{3cm}

```{r logo facultad, echo=F, include = T, out.width= "60%"}
knitr::include_graphics("logounr.png")
```

\pagenumbering{gobble}

\vspace{5cm}

\Large
**LICENCIATURA EN ESTADÍSTICA**

\vspace{1cm}

\Large
**METRÓPOLIS-HASTINGS**


\vspace{0.3cm}
\large

*"Markov chain Monte Carlo (MCMC) method"*

\vspace{9cm}

\large

**Autores: Franco Santini - Nicolas Gamboa - Andrés Roncaglia**

**Docentes: Ignacio Evangelista - Tomás Capretto**

**2024**
\normalsize
\newpage
\hypersetup{linkcolor = black}
\tableofcontents


\newpage
\pagenumbering{arabic}

:::

\newpage


# Introducción

Uno de los grandes problemas en el mundo de la estadística es la existencia de distribuciones cuyas funciones de densidad son tan complejas que resultan difíciles de trabajar. Es por esto que muchas veces resulta de utilidad trabajar con muestras aleatorias, y a partir de estas responder determinadas preguntas. En la práctica, es común encontrarse con variables aleatorias con funciones de densidad cuyo muestreo directo no es simple.

Para estos casos existen distintas técnicas que permiten obtener muestras que, si bien no son tomadas de manera realmente independiente, se comportan de manera muy similar a como lo haría una muestra aleatoria independiente tomada de la función de densidad. 

Particularmente en el campo de la _Estadística Bayesiana_, esto resulta útil ya que permite obtener muestras de la distribución a posteriori, la cual se puede utilizar, por ejemplo, para obtener un intervalo de credibilidad del parámetro bajo estudio.

Un método sencillo de emplear, y que reporta buenos resultados, es el algoritmo de [_Metropolis-Hastings_](https://es.wikipedia.org/wiki/Algoritmo_de_Metropolis-Hastings). 


# Metodología

Si se quiere obtener una muestra de la función de densidad $f(x)$, de la cual no es sencillo obtener muestras, el algoritmo de Metropolis-Hastings plantea que proponiendo una función $q(x)$ conocida, de la cual es sencillo muestrear, y un punto inicial ($\theta_0$), es posible obtener muestras de la función $f(x)$. 

El método funciona de la siguiente forma:

- De la distribución $q(x)$ centrada en $\theta_0$ muestrear un nuevo punto ($\theta'$)

- Si la densidad de la función $f(x)$ es mayor en $\theta'$ que en $\theta_0$, entonces $\theta'$ formará parte de la muestra, en caso contrario será parte de la muestra con probabilidad $\frac{f(\theta')}{f(\theta_0)}\frac{q(\theta_0 \ |\theta')}{q(\theta' \ |\theta_0)}$ o con probabilidad \newline $1-\frac{f(\theta')}{f(\theta_0)}\frac{q(\theta_0 \ |\theta')}{q(\theta' \ |\theta_0)}$ se repetirá $\theta$ en la muestra.

- Repetir el proceso, obteniendo nuevos puntos propuestos a partir de $q(x)$ centrada en el último valor muestreado.

En definitiva, la probabilidad de seleccionar $\theta'$ como nuevo valor de la muestra es $\alpha = min\left\{1;\frac{f(\theta')}{f(\theta_0)}\frac{q(\theta_0 \ |\theta')}{q(\theta' \ |\theta_0)}\right \}$

```{r Ejemplo M-H}
#| fig-cap: "Ejemplificación de la obtención de una muestra por el método de Metropolis-Hastings"

x <- seq(-4,4,0.01)
f <- exp(0.4*(x-0.4)^2-0.08*x^4)

ggplot(tibble(x,f)) +
  geom_line(aes(x=x,y=f),linewidth=1,col="dodgerblue3") +
  geom_ribbon(aes(x=x,ymin=0,ymax=f),alpha=0.6,fill="dodgerblue3") +
  xlab(expression(theta)) +
  theme(axis.title.y = element_blank()) +
  geom_line(data=tibble(x=seq(-4,2,0.01),y=dnorm(x,-1,0.8)),
            aes(x=x,y=y), col="#ff3d58") +
  geom_ribbon(data=tibble(x=seq(-4,2,0.01),y=dnorm(x,-1,0.8)),
              aes(x=x,ymin=0,ymax=y), fill="#ff3d58",alpha=0.6) +
  geom_point(aes(x=-1,y=0),shape="square",size=2.5) +
  geom_point(aes(x=0.4,y=0),shape=17,size=2.5) +
  geom_curve(aes(x=-1,xend=0.35,y=0.1,yend=0.1),
             curvature = -0.5,
             arrow = arrow(length = unit(0.02, "npc"),type="closed"),
             col="black") +
  geom_label(aes(x=-2.6, y=2.25),
             label=expression(f(x)),
             fill="dodgerblue3",
             size=5 ,fontface="bold") +
  geom_label(aes(x=-2, y=0.3),
             label=expression(q(x)),
             fill="#ff3d58",
             size=5 ,fontface="bold") +
  annotate(geom = "text", x=c(-0.95, 0.4), y=c(-0.18, -0.14), label=c(expression(theta[0]), expression(theta^"|"))) +
  geom_segment(aes(x=0.4,xend=0.4,y=0,yend=exp(0.4*(0.4-0.4)^2-0.08*(0.4)^4)),linetype="dashed") +
  geom_segment(aes(x=-1,xend=-1,y=0,yend=exp(0.4*(-1-0.4)^2-0.08*(-1)^4)),linetype="dashed") +
  geom_segment(aes(x=-1,xend=-4,y=exp(0.4*(-1-0.4)^2-0.08*(-1)^4),yend=exp(0.4*(-1-0.4)^2-0.08*(-1)^4)),linetype="dotted") +
  geom_segment(aes(x=0.4,xend=-4,y=exp(0.4*(0.4-0.4)^2-0.08*(0.4)^4),yend=exp(0.4*(0.4-0.4)^2-0.08*(0.4)^4)),linetype="dotted")

```

A continuación se presentan las funciones usadas para aplicar el método en los casos en los que $f$ es unidimensional y bidimensional:

::: {.callout-note}

## Algoritmo Metrópolis-Hastings univariado

```{r algoritmo mh univariado doc 1, echo = T, eval=FALSE}
sample_mh <- function(n, d_objetivo, r_propuesta = NULL, 
                      d_propuesta = NULL, p_inicial = NULL){
  
  # Posibles errores al llamar a la función
  if (length(p_inicial) != 1) {
    stop("El valor p_inicial debe ser unidimensional")
  }
  if ( n <= 0 || n %% 1 != 0) {
    stop("El tamaño de muestra n debe ser entero y mayor que 0")
  }
  
  # En caso de no definir una distribución propuesta se utiliza
  # una normal con varianza igual a 1
  if (is.null(r_propuesta) | is.null(d_propuesta)) {
    r_propuesta <- function(media) rnorm(n = 1, media, sd = 1)
    d_propuesta <- function(x, media) dnorm(x = x,media, sd = 1)
  }
```

:::

\newpage

:::{.callout-note}
##       
```{r algoritmo mh univariado doc 2, echo = T, eval=FALSE}
  # Se definen valores iniciales
  stopifnot(n > 0)
  contador <- 0
  muestras <- numeric(n)
  muestras[1] <- p_inicial

  # Iteraciones para obtener las n-1 muestras restantes
  for(i in 2:n) {
    # Se define el valor actual, y el nuevo valor propuesto
    p_actual <- muestras[i-1]
    p_propuesta <- r_propuesta(p_actual)
    
    # Se calculan las densidades de estos 
    # valores para las distribuciones propuesta y objetivo 
    q_actual <- d_propuesta(p_actual, p_propuesta)
    q_nuevo <- d_propuesta(p_propuesta, p_actual)
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_propuesta)

    # Si la densidad del valor actual para la distribución obj es 0,
    # se elige el nuevo valor propuesto con probabilidad 1
    if (f_actual == 0 || q_nuevo == 0) {
      alfa <- 1
    } else {
      alfa <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    }
    
    # Se elige el nuevo valor de la muestra con una probabilidad alfa
    if (aceptar) {
      muestras[i] <- p_propuesta
    }else{
      muestras[i] <- p_actual
    }

    # Se actualiza el número de saltos aceptados
    if(muestras[i] != muestras[i-1]) {
      contador <- contador + 1
    }
  }
  # Devuelve una lista con 2 elementos. Un data frame con la
  # muestra y la tasa de aceptación.
  return(list(cadena = data.frame(iteracion = 1:n, x = muestras), 
       tasa_aceptacion = contador / n))
}
```

:::

\newpage

Con el propósito de facilitar los cálculos de la técnica, no se especificará en los argumentos la forma de la distribución propuesta, en cambio, $q(\vec x)$ será Normal, cuya matriz de covarianzas $\Sigma$ y valor inicial $\vec \theta_0$ serán los únicos argumentos en relación a la distribución propuesta que se brindarán en la función para la obtención de muestras por Metropolis-Hastings para distribuciones bivariadas. 


::: {.callout-note}

## Algoritmo Metrópolis-Hastings bivariado

```{r algoritmo m-h bivariado doc 1, echo = T, eval = F}
sample_mh_mv <- function(n, d_objetivo, cov_propuesta = diag(2), 
                         p_inicial = numeric(2)) {
  
  # Posibles errores al llamar a la función
  if (length(p_inicial) != 2) {
    stop("El valor p_inicial debe ser bidimensional")
  }
  if ( n <= 0 || n %% 1 != 0) {
    stop("El tamaño de muestra n debe ser entero y mayor que 0")
  }
  if (any((dim(cov_propuesta) != c(2,2)))) {
    stop("La matriz de covariancia debe ser de 2x2")
  }
  
  # Distribuciones propuestas a utilizar
  r_propuesta <-  function(media) rmvnorm(n = 1,mean = media,sigma = cov_propuesta)
  d_propuesta <- function(x, media) dmvnorm(x = x,mean = media,sigma = cov_propuesta)
  
  # Se definen valores iniciales
  contador <- 0
  muestras <- matrix(0,nrow = n,ncol = length(p_inicial))
  muestras[1, ] <- p_inicial
  
  # Iteraciones para obtener las n-1 muestras restantes
  for(i in 2:n) {
    # Se define el valor actual, y el nuevo valor propuesto
    p_actual <- muestras[i-1,]
    p_propuesta <- r_propuesta(p_actual)
    
    # Se calculan las densidades de estos 
    # valores para las distribuciones propuesta y objetivo 
    q_actual <- d_propuesta(p_actual, p_propuesta)
    q_nuevo <- d_propuesta(p_propuesta, p_actual)
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_propuesta)
```

:::

\newpage

:::{.callout-note}

##    
```{r algoritmo m-h bivariado doc 2, echo = T, eval = F}

# Si la densidad del valor actual para la distribución obj es 0,
    # se elige el nuevo valor propuesto con probabilidad 1
    if (f_actual == 0 || q_nuevo == 0) {
      alfa <- 1
    } else {
      alfa <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    }
    

    # Se elige el nuevo valor de la muestra con una probabilidad alfa
    muestras[i] <- sample(
      c(p_propuesta, p_actual),
      size = 1, 
      prob = c(alfa, 1-alfa)
    )
    
    
    # Se actualiza el número de saltos aceptados
    if(!any(muestras[i,] != muestras[i-1,])) {
      contador <- contador + 1
    }
  }
  
  salida <- data.frame(iteracion = 1:n, x = muestras) |> 
    `colnames<-`(c("iteracion", paste0("dim_",1:length(p_inicial))))
  
  # Devuelve una lista con 2 elementos. Un data frame con la
  # muestra y la tasa de aceptación.
  return(list(muestra_mh = salida,
       probabilidad_aceptacion = contador / n))

}
```

:::

\newpage

# Discusiones

## Metropolis-Hastings en una dimensión

### Distribución de Kumaraswamy

La distribución de _Kumaraswamy_ es una distribución de probabilidad continua que se utiliza para modelar variables aleatorias con soporte en el intervalo $(0,1)$, cuya función de densidad es: \newline $f(x|a,b) = abx^{a-1}(1-x^a)^{b-1}, \ con \ a,b>0$



```{r distribucion de kumaraswamy}
#| fig-cap: "Función de densidad de la distribución de Kumaraswamy"

grilla <- seq(0, 1, length.out = 502)[c(-1,-502)]
d_kuma <- function(x, a = 6, b = 2){
  
  if(a<0 | b<0) stop("Poneme a y b positivos no te cuesta nada")
  
  salida <- a*b*x^(a-1)*(1-x^a)^(b-1)
  
  salida[which(x <= 0 | x >= 1)] <- 0
  
  salida
}

tibble(
  x = rep(grilla,times = 5),
  Parámetros = rep(c("a = 0.5, b = 0.5",
                     "a = 1, b = 2",
                     "a = 3, b = 4",
                     "a = 6, b = 2",
                     "a = 5, b = 1"), each = 500),
  valores = c(
    d_kuma(grilla, 0.5, 0.5),
    d_kuma(grilla, 1, 2),
    d_kuma(grilla, 3, 4),
    d_kuma(grilla, 6, 2),
    d_kuma(grilla, 5,1)
  )
) |> 
  ggplot()+
  aes(x = x, y = valores,color = Parámetros)+
  geom_line(linewidth = 1.2) +
  labs(x = "x", y = expression(f ~ "("~x~" | a, b)"))
```

Esta distribución puede ser utilizada en el ámbito de la estadística bayesiana a la hora de definir un prior para un parámetro con campo de variación en el intervalo (0, 1).

Por lo general la distribución elegida para estas situaciones suele ser la $Beta$^[$p(x| \ a,b) = \frac{x^{a-1}(1-x)^{b-1}}{B(a,b)}$ con $B(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$] ya que presenta ventajas como ser una distribución conjugada de la binomial, lo cual puede facilitar mucho algunos cálculos. El problema es que la densidad de esta depende de la función _gamma_^[$\Gamma(x)=\int^\infty_0 u^{x-1}e^{-u}du$], la cual es una integral, y en algunas situaciones se puede complicar su cálculo.

La distribución de Kumaraswamy se comporta de manera muy similar a la beta, sin tener el problema de la dificultad del cálculo de la integral. 


\newpage

Para probar la calidad de las muestras obtenidas por Metropolis-Hastings, se generan 5000 muestras de la distribución de _Kumaraswamy_ con parámetros $a = 6$ y $b = 2$, utilizando como distribución propuesta una $Beta(\mu, \kappa)$, donde $\mu$ representa la media de la distribución y $\kappa$ el grado de la concentración de la distribución. Se realiza este procedimiento para 3 valores distintos de $\kappa$.

```{r comparacion de las cadenas}
get_beta_param <- function(media, kappa) {
  alpha <- media*kappa
  beta <- (1-media)*kappa
  
  return(list(alpha = alpha, beta = beta))
}
```


```{r cadenas markov, include=F}
valores_kappa <- c(1, 2, 5)

graficos <- list(Kappa1 = NULL, Kappa2 = NULL, Kappa5 = NULL)

df_cadenas <- list(data_cadena_1 = NULL, data_cadena_2 = NULL, data_cadena_3 = NULL)

for(i in 1:3) {
  r_prop <- function(x) {
  kappa <- valores_kappa[i]
  pars <- get_beta_param(x, kappa)
  rbeta(1, shape1 = pars$alpha, shape2 = pars$beta)
}
d_prop <- function(x, mean) {
  kappa <- valores_kappa[i]
  pars <- get_beta_param(mean, kappa)
  dbeta(x, shape1 = pars$alpha, shape2 = pars$beta)
}

muestra <- sample_mh(n = 5000, d_objetivo = d_kuma, r_propuesta = r_prop, d_propuesta = d_prop, p_inicial = rbeta(1,2,2))

ap1 <- plot_autocor(muestra = muestra$cadena)

tp1 <- plot_trace(muestra = muestra$cadena)

hp1 <- plot_hist(muestra = muestra$cadena, d_objetivo = d_kuma)

graficos[[i]] <- (tp1 / (hp1 + ap1))

df_cadenas[[i]] <- muestra
}

neff1 <- n_eff(df_cadenas$data_cadena_1$cadena[2])
neff2 <- n_eff(df_cadenas$data_cadena_2$cadena[2])
neff3 <- n_eff(df_cadenas$data_cadena_3$cadena[2])

```


```{r cadena 1}
#| fig-cap: "Muestreo por Metropolis-Hastings con $\\kappa$ = 1"

graficos$Kappa1
```


```{r cadena 2}
#| fig-cap: "Muestreo por Metropolis-Hastings con $\\kappa$ = 2"

graficos$Kappa2
```


```{r cadena 3}
#| fig-cap: "Muestreo por Metropolis-Hastings con $\\kappa$ = 5"

graficos$Kappa5
```

Sin embargo, debido a que los valores propuestos ($\theta'$) en las iteraciones del método son obtenidos a partir de una distribución centrada en el valor anterior muestreado ($\theta_0$), las muestras están correlacionadas. Resulta de interés entonces conocer a cuantas muestras independientes equivalen, dado que cuanto más se comporta una cadena de Markov dependiente como una muestra independiente, menor será el error en la aproximación del posterior resultante. Con este objetivo se calcula el número efectivo de muestras $(n_{eff})$ para cada cadena.

```{r tabla nro efectivo de muestra}
cadenas_text <- c("Cadena 1", "Cadena 2", "Cadena 3")

nro_ef_muestras <- tibble(
  Cadena = cadenas_text,
  kappa = rep(c(1,2,5)),
  n_eff = floor(c(neff1,neff2,neff3))
) |> `colnames<-`(c("Cadena", "$\\kappa$", "$n_{eff}$"))
```


```{r tabla nro efectivo de muestra 2}
#| tbl-cap: "Número efectivo de muestras para cadenas de 5000 muestras y distintos valores de $\\kappa$"
#| tbl-pos: H
#| tbl-cap-location: bottom

nro_ef_muestras |> 
  tt() 
```

Se concluye que el valor de $\kappa$ que arroja una muestra relacionada que equivale a una independiente de mayor tamaño es 5. Sin embargo, esta cantidad de muestras independientes es muy pobre teniendo en cuenta que el tamaño de la cadena es de 5000 muestras.


```{r funcion x}

# Creación de la tabla

# Cadena 1
media_cadena_1 <- mean(df_cadenas$data_cadena_1$cadena$x) |> round(3)
percentiles_1 <- quantile(df_cadenas$data_cadena_1$cadena$x, c(0.05,0.95)) |> round(3)

# Cadena 2
media_cadena_2 <- mean(df_cadenas$data_cadena_2$cadena$x) |> round(3)
percentiles_2 <- quantile(df_cadenas$data_cadena_2$cadena$x, c(0.05,0.95)) |> round(3)

# Cadena 3
media_cadena_3 <- mean(df_cadenas$data_cadena_3$cadena$x) |> round(3)
percentiles_3 <- quantile(df_cadenas$data_cadena_3$cadena$x, c(0.05,0.95)) |> round(3)

# Media y cuantiles verdaderos

g <- function(x) x*d_kuma(x, 6, 2)
media_kumar <- integrate(g, 0, 1)$value |> round(digits = 3)

kumar_05 <- qkumar(p = 0.05, shape1 = 6, shape2 = 2) |> round(digits = 3)
kumar_95 <- qkumar(p = 0.95, shape1 = 6, shape2 = 2) |> round(digits = 3)

logit <- function(x) log(x/(1-x))

# Cadena 1
media_cadena_lg_1 <- mean(logit(df_cadenas$data_cadena_1$cadena$x)) |> round(3)
percentiles_lg_1 <- quantile(logit(df_cadenas$data_cadena_1$cadena$x), c(0.05,0.95)) |> round(3)

# Cadena 2
media_cadena_lg_2 <- mean(logit(df_cadenas$data_cadena_2$cadena$x)) |> round(3)
percentiles_lg_2 <- quantile(logit(df_cadenas$data_cadena_2$cadena$x), c(0.05,0.95)) |> round(3)

# Cadena 3
media_cadena_lg_3 <- mean(logit(df_cadenas$data_cadena_3$cadena$x)) |> round(3)
percentiles_lg_3 <- quantile(logit(df_cadenas$data_cadena_3$cadena$x), c(0.05,0.95)) |> round(3)

# Media y cuantiles verdaderos
g_lg <- function(x) logit(x)*(d_kuma(x, 6, 2))

media_kumar_lg <- integrate(g_lg, 0, 1)$value |> round(digits = 3)
kumar_05_lg <- logit(qkumar(p = 0.05, shape1 = 6, shape2 = 2)) |> round(digits = 3)
kumar_95_lg <- logit(qkumar(p = 0.95, shape1 = 6, shape2 = 2)) |> round(digits = 3)

```

\newpage


Resulta de interés ver como afecta la elección del parámetro $\kappa$ al utilizar Metropolis-Hastings para obtener muestras de la distribución de Kumaraswamy usando una distribución $Beta$ como propuesta. Es por esto que se decide obtener la media y ciertos cuantiles sobre las muestras obtenidas y sobre la función Logit de estas.

```{r tablita fachera}
filas <- rep(c("$X$", "$Logit(X)$"), each = 3)

valores <- tibble(
  var = filas,
  kappa = rep(c("1","2","5"), times = 2), 
  esp_est = c(media_cadena_1, media_cadena_2, media_cadena_3, media_cadena_lg_1, media_cadena_lg_2, media_cadena_lg_3),
  quantil_0.05 = c(percentiles_1[1], percentiles_2[1], percentiles_3[1], percentiles_lg_1[1], percentiles_lg_2[1], percentiles_lg_3[1]),
  quantil_0.95 = c(percentiles_1[2], percentiles_2[2], percentiles_3[2], percentiles_lg_1[2], percentiles_lg_2[2], percentiles_lg_3[2])
) |> 
  `colnames<-`(c("$f(x)$" ,"$\\kappa$", "$\\hat{E(x)}$", "$\\hat{q_{0.05}}$", "$\\hat{q_{0.95}}$"))
```


```{r tablita fachera 2}
#| tbl-cap: "Media y quantiles estimadas para las funciones $X$ y $Logit(X)$"
#| tbl-pos: H
#| tbl-cap-location: bottom

valores |> 
  tt() |> 
  style_tt(i = c(1,4), j = 1, rowspan = 3, alignv = "t")
```


Las estadísticas reales de la distribución de Kumaraswamy son:

```{r valores reales kuma}
filas <- c("$X$", "$Logit(X)$")

valores_reales <- tibble(
  var = filas,
  esp_est = c(media_kumar,media_kumar_lg),
  quantil_0.05 = c(kumar_05,kumar_05_lg),
  quantil_0.95 = c(kumar_95,kumar_95_lg)
) |> 
  `colnames<-`(c("$f(x)$" ,"$E(x)$", "$q_{0.05}$", "$q_{0.95}$"))
```


```{r valores reales kuma 2}
#| tbl-cap: "Media y quantiles para las funciones $X$ y $Logit(X)$"
#| tbl-pos: H
#| tbl-cap-location: bottom

valores_reales |> 
  tt()
```

Si bien las diferencias en dispersión de las muestras no son muy perceptibles a simple vista, gracias a la funcióm logit podemos percibirlas con mayor facilidad. Es así que se puede concluir que usando una distribución $Beta$ con un parámetro $\kappa = 2$ de concentración, es que se obtienen las estimaciones más cercanas a la distribución de Kumaraswamy con parámetros $a = 6$ y $b = 2$. 

\newpage

## Metropolis-Hastings en dos dimensiones

### Normal Bivariada

El algoritmo de Metropolis-Hastings es más apreciable cuando se necesita obtener muestras de distribuciones de más de una dimensión.

Para testear el algoritmo de Metropolis-Hastings bivariado creado se decide obtener muestras de una distribución Normal bivariada con vector de media $\mu^*$ y matriz de covarianza $\Sigma^*$.

Donde $\mu^* = \begin{bmatrix} 0.4 \\ 0.75 \end{bmatrix}$ y $\Sigma^* = \begin{bmatrix} 1.35 & 0.4 \\ 0.4 & 2.4 \end{bmatrix}$.

Se proponen 3 matrices de covarianzas distintas para observar con cuál de estas se obtienen muestras más aproximadas a la verdadera distribución:

1. $\Sigma_1 = \begin{bmatrix} 1 & 0 \\ 0 & 2 \end{bmatrix}$ Se puede ver su densidad en la @fig-cov1, el recorrido de la cadena en la @fig-trace-cov1 y su autocorrelación en la @fig-autocor1.

2. $\Sigma_2 = \begin{bmatrix} 0.25 & 0 \\ 0 & 0.25 \end{bmatrix}$ Se puede ver su densidad en la @fig-cov2, el recorrido de la cadena en la @fig-trace-cov2 y su autocorrelación en la @fig-autocor2.

3. $\Sigma_3 = \begin{bmatrix} 1 & 0.3 \\ 0.3 & 1 \end{bmatrix}$ Se puede ver su densidad en la @fig-cov3, el recorrido de la cadena en la @fig-trace-cov3 y su autocorrelación en la @fig-autocor3.

```{r cadenas de markov bivariadas}
mu <- c(.4,.75) 
sigma <- matrix(c(1.35,.4,.4,2.4),2)
d_obj <- function(x) dmvnorm(x,mean = mu,sigma = sigma)

cov_prop_1 <- diag(c(1,2))
cov_prop_2 <- diag(c(0.25,0.25))
cov_prop_3 <- matrix(c(1,0.3,0.3,1), ncol = 2)

cadena1 <- sample_mh_mv(n = 5000,d_objetivo = d_obj, cov_propuesta = cov_prop_1, p_inicial = c(0,0))
cadena2 <- sample_mh_mv(n = 5000,d_objetivo = d_obj, cov_propuesta = cov_prop_2, p_inicial = c(0,0))
cadena3 <- sample_mh_mv(n = 5000,d_objetivo = d_obj, cov_propuesta = cov_prop_3, p_inicial = c(0,0))

```



```{r distribucion normal bivariada 1}
#| fig-cap: "Distribución de las muestras obtenidas por Metropolis-Hastings de una $\\mathcal{N}_2(\\mu^*, \ \\Sigma^*)$ con $\\Sigma_1$ como propuesta"
#| label: fig-cov1

plot_hotmap(muestra = cadena1$muestra_mh, d_objetivo = d_obj, puntos = F)
```

```{r traceplot normal bivariada 1}
#| fig-cap: "Cadena de Markov de las muestras obtenidas por Metropolis-Hastings de una $\\mathcal{N}_2(\\mu^*, \ \\Sigma^*)$ con $\\Sigma_1$ como propuesta"
#| label: fig-trace-cov1

plot_trace(muestra = cadena1$muestra_mh)
```


```{r autocorrelacion normal bivariada 1}
#| fig-cap: "Autocorrelación de las muestras obtenidas por Metropolis-Hastings de una $\\mathcal{N}_2(\\mu^*, \ \\Sigma^*)$ con $\\Sigma_1$ como propuesta"
#| label: fig-autocor1

plot_autocor(cadena1$muestra_mh)
```

```{r distribucion normal bivariada 2}
#| fig-cap: "Distribución de las muestras obtenidas por Metropolis-Hastings de una $\\mathcal{N}_2(\\mu^*, \ \\Sigma^*)$ con $\\Sigma_2$ como propuesta"
#| label: fig-cov2

plot_hotmap(muestra = cadena2$muestra_mh, d_objetivo = d_obj, puntos = F)
```


```{r traceplot normal bivariada 2}
#| fig-cap: "Cadena de Markov de las muestras obtenidas por Metropolis-Hastings de una $\\mathcal{N}_2(\\mu^*, \ \\Sigma^*)$ con $\\Sigma_2$ como propuesta"
#| label: fig-trace-cov2

plot_trace(muestra = cadena2$muestra_mh)
```

```{r autocorrelacion normal bivariada 2}
#| fig-cap: "Autocorrelación de las muestras obtenidas por Metropolis-Hastings de una $\\mathcal{N}_2(\\mu^*, \ \\Sigma^*)$ con $\\Sigma_2$ como propuesta"
#| label: fig-autocor2

plot_autocor(cadena2$muestra_mh)
```


```{r distribucion normal bivariada 3}
#| fig-cap: "Distribución de las muestras obtenidas por Metropolis-Hastings de una $\\mathcal{N}_2(\\mu^*, \ \\Sigma^*)$ con $\\Sigma_3$ como propuesta"
#| label: fig-cov3

plot_hotmap(muestra = cadena3$muestra_mh, d_objetivo = d_obj, puntos = F)
```


```{r traceplot normal bivariada 3}
#| fig-cap: "Cadena de Markov de las muestras obtenidas por Metropolis-Hastings de una $\\mathcal{N}_2(\\mu^*, \ \\Sigma^*)$ con $\\Sigma_3$ como propuesta"
#| label: fig-trace-cov3

plot_trace(muestra = cadena3$muestra_mh)
```

```{r autocorrelacion normal bivariada 3}
#| fig-cap: "Autocorrelación de las muestras obtenidas por Metropolis-Hastings de una $\\mathcal{N}_2(\\mu^*, \ \\Sigma^*)$ con $\\Sigma_3$ como propuesta"
#| label: fig-autocor3

plot_autocor(cadena3$muestra_mh)
```

```{r tabla numero efectivo de muestras normal bivariada 1}
cadena_muestra <- c("Cadena 1", "Cadena 2", "Cadena 3")
neff_1 <- n_eff(cadena1$muestra_mh[,-1])
neff_2 <- n_eff(cadena2$muestra_mh[,-1])
neff_3 <- n_eff(cadena3$muestra_mh[,-1])


neff_tabla <- tibble(
  var = cadena_muestra,
  dim_1 = c(floor(neff_1[1]), floor(neff_2[1]), floor(neff_3[1])),
  dim_2 = c(floor(neff_1[2]), floor(neff_2[2]), floor(neff_3[2]))
) |> `colnames<-`(c("Cadena","$x_1$", "$x_2$"))


```

```{r tabla numero efectivo de muestras normal bivariada 2}
#| tbl-cap: "Número efectivo de muestras para cadenas de 5000 muestras y distintos $\\Sigma$"
#| tbl-pos: H

neff_tabla |> 
  tt()
```

Considerando los tamaños de muestras efectivos en cada dimensión y los gráficos de autocorrelación observados, se puede concluir que con la matriz de covarianzas $\Sigma_1$ se obtienen muestras aparentemente mejores de la distribución objetivo.

Para ver la bondad de la muestra generada por Metropolis-Hastings, se calculan ciertas probabilidades y luego se comparan con otros métodos de cálculo de probabilidades, como "Función de distribución" y "Método de MonteCarlo".

```{r calculo de las probabilidades normal bivariada estimadas}
# i
probabilidad_p8_1 <- mean(cadena1$muestra_mh$dim_1 > 1 & cadena1$muestra_mh$dim_2 < 0) |> round(4)

# ii
probabilidad_p8_2 <- mean(cadena1$muestra_mh$dim_1 > 1 & cadena1$muestra_mh$dim_2 > 2) |> round(4)

# iii
probabilidad_p8_3 <- mean(cadena1$muestra_mh$dim_1 > 0.4 & cadena1$muestra_mh$dim_2 > 0.75) |> round(4)

# i funcion de distribucion
prob_1 <- as.numeric(pmvnorm(lower = c(1, -Inf), upper = c(Inf, 0), mean = mu, sigma = sigma)) |> round(4)

# ii funcion de distribucion
prob_2 <- as.numeric(pmvnorm(lower = c(1, 2), upper = c(Inf, Inf), mean = mu, sigma = sigma)) |> round(4)

# iii funcion de distribucion
prob_3 <- as.numeric(pmvnorm(lower = c(0.4, 0.75), upper = c(Inf, Inf), mean = mu, sigma = sigma)) |> round(4)

muestras_normal_bivariada <- rmvnorm(5000, mean = c(0.4, 0.75), sigma = matrix(c(1.35,.4,.4,2.4),2)) |> round(4)

# i MCMC
prob_1_mcmc <- mean(muestras_normal_bivariada[,1] > 1 & muestras_normal_bivariada[,2] < 0) |> round(4)

# ii MCMC
prob_2_mcmc <- mean(muestras_normal_bivariada[,1] > 1 & muestras_normal_bivariada[,2] > 2) |> round(4)

# iii MCMC
prob_3_mcmc <- mean(muestras_normal_bivariada[,1] > 0.4 & muestras_normal_bivariada[,2] > 0.75) |> round(4)

```

```{r tabla para probabilidad p8}
filas <- c("M-H", "Función de \ndistribución", "MC")

probabilidades_nb <- tibble(
  var = filas,
  p1 = c(probabilidad_p8_1, prob_1, prob_1_mcmc),
  p2 = c(probabilidad_p8_2, prob_2, prob_2_mcmc),
  p3 = c(probabilidad_p8_3, prob_3, prob_3_mcmc)
) |> `colnames<-`(c("Método","$P(X_1 > 1, X_2 < 0)$", "$P(X_1 > 1, X_2 > 2)$", "$P(X_1 > 0.4, X_2 > 0.75)$"))
```

```{r tabla probabilidades 1}
#| tbl-cap: "Cálculo de probablidades con distintos métodos"

probabilidades_nb |> 
  tt() 
```

Se puede concluir que las estimaciones de las probabilidades calculadas son bastante acertadas, debido a su similitud con los otros métodos.

\newpage

### Función de Rosenbrock

Es una función matemática, popularmente conocida como *La banana de Rosenbrock* utilizada frecuentemente como prueba de algoritmos de optimización numérica.

$$f(x,y) = (a-x)^2+b(y-x^2)^2$$

En el campo de la Estadística Bayesiana, es muy conocida dado que la densidad del posterior muchas veces toma una forma que se asemeja a la función de *Rosenbrock*.

Un ejemplo de este fenómeno es la función de densidad que viene dada por: \newline $p^*(x_1,x_2|a,b) = exp\{-[(a-x_1)^2+b(x_2-x_1^2)^2]\}, \ a,b \in \mathbb{R}$

Para poner a prueba el algoritmo de Metropolis-Hastings bivariado en situaciones complejas, se decide obtener 5000 muestras de la función anteriormente mencionada, con parámetros $a = 0.5$ y $b = 5$. Se utilizan 3 distintas matrices de covarianzas propuestas, las cuáles son:

```{r banana de rosenbrock 1}
d_obj_p <- function(x, a = 0.5, b = 5) exp(-((a-x[1])^2 + b *(x[2]-x[1]^2)^2))

cov_prop_p1 <- diag(c(0.06, 0.07))

cadena_p1 <- sample_mh_mv(n = 5000, d_objetivo = d_obj_p, cov_propuesta = cov_prop_p1, p_inicial = c(0,0))
```

1. $\Sigma_1 = \begin{bmatrix} 0.06 & 0 \\ 0 & 0.07 \end{bmatrix}$ Se puede ver su densidad en la @fig-banana1, el recorrido de la cadena en la @fig-bananatrace1 y la autocorrelación en la @fig-bananacor1. Con esta matriz la probabilidad de aceptación es `r round(cadena_p1$probabilidad_aceptacion, 4)`.

```{r banana de rosenbrock 2}
cov_prop_p2 <- diag(c(1, 1))

cadena_p2 <- sample_mh_mv(n = 5000, d_objetivo = d_obj_p, cov_propuesta = cov_prop_p2, p_inicial = c(0,0))
```

2. $\Sigma_2 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ Se puede ver su densidad en la @fig-banana2, el recorrido de la cadena en la @fig-bananatrace2 y la autocorrelación en la @fig-bananacor2. Con esta matriz la probabilidad de aceptación es `r round(cadena_p2$probabilidad_aceptacion, 4)`.

```{r banana de rosenbrock 3}
cov_prop_p3 <- diag(c(0.5, 0.5))

cadena_p3 <- sample_mh_mv(n = 5000, d_objetivo = d_obj_p, cov_propuesta = cov_prop_p3, p_inicial = c(0,0))
```


3. $\Sigma_3 = \begin{bmatrix} 0.5 & 0 \\ 0 & 0.5 \end{bmatrix}$ Se puede ver su densidad en la @fig-banana3, el recorrido de la cadena en la @fig-bananatrace3 y la autocorrelación en la @fig-bananacor3. Con esta matriz la probabilidad de aceptación es `r round(cadena_p3$probabilidad_aceptacion, 4)`.


```{r hotmap de rosenbrock 1}
#| fig-cap: "Distribución de las muestras obtenidas por Metropolis-Hastings de $p^*$ con $\\Sigma_1$ como propuesta"
#| label: fig-banana1

plot_hotmap(cadena_p1$muestra_mh[-1], d_objetivo = d_obj_p, puntos = F)
```

```{r traceplot banana de rosenbrock 1}
#| fig-cap: "Cadena de Markov de las muestras obtenidas por Metropolis-Hastings de $p^*$ con $\\Sigma_1$ como propuesta"
#| label: fig-bananatrace1

plot_trace(cadena_p1$muestra_mh)
```

```{r autocorrelacion banana de rosenbrock 1}
#| fig-cap: "Autocorrelación de las muestras obtenidas por Metropolis-Hastings de $p^*$ con $\\Sigma_1$ como propuesta"
#| label: fig-bananacor1

plot_autocor(cadena_p1$muestra_mh)
```

```{r hotmap de rosenbrock 2}
#| fig-cap: "Distribución de las muestras obtenidas por Metropolis-Hastings de $p^*$ con $\\Sigma_2$ como propuesta"
#| label: fig-banana2

plot_hotmap(cadena_p2$muestra_mh[-1], d_objetivo = d_obj_p, puntos = F)
```

```{r traceplot banana de rosenbrock 2}
#| fig-cap: "Cadena de Markov de las muestras obtenidas por Metropolis-Hastings de $p^*$ con $\\Sigma_2$ como propuesta"
#| label: fig-bananatrace2

plot_trace(cadena_p2$muestra_mh)
```

```{r autocorrelacion banana de rosenbrock 2}
#| fig-cap: "Autocorrelación de las muestras obtenidas por Metropolis-Hastings de $p^*$ con $\\Sigma_2$ como propuesta"
#| label: fig-bananacor2

plot_autocor(cadena_p2$muestra_mh)
```

```{r banana de rosenbrock 3}
#| fig-cap: "Distribución de las muestras obtenidas por Metropolis-Hastings de $p^*$ con $\\Sigma_3$ como propuesta"
#| label: fig-banana3

plot_hotmap(cadena_p3$muestra_mh[-1], d_objetivo = d_obj_p, puntos = F)
```

```{r banana de rosenbrock 3}
#| fig-cap: "Cadena de Markov de las muestras obtenidas por Metropolis-Hastings de $p^*$ con $\\Sigma_3$ como propuesta"
#| label: fig-bananatrace3

plot_trace(cadena_p3$muestra_mh)
```

```{r autocorrelacion banana de rosenbrock 3}
#| fig-cap: "Autocorrelación de las muestras obtenidas por Metropolis-Hastings de $p^*$ con $\\Sigma_3$ como propuesta"
#| label: fig-bananacor3

plot_autocor(cadena_p3$muestra_mh)
```

```{r tabla numero efectivo de muestras rosenbrock 1}
cadena_muestra_ros <- c("Cadena 1", "Cadena 2", "Cadena 3")
neff_1_ros <- n_eff(cadena_p1$muestra_mh[,-1])
neff_2_ros <- n_eff(cadena_p2$muestra_mh[,-1])
neff_3_ros <- n_eff(cadena_p3$muestra_mh[,-1])

neff_tabla_ros <- tibble(
  var = cadena_muestra_ros,
  dim_1 = c(floor(neff_1_ros[1]), floor(neff_2_ros[1]), floor(neff_3_ros[1])),
  dim_2 = c(floor(neff_1_ros[2]), floor(neff_2_ros[2]), floor(neff_3_ros[2]))
) |> `colnames<-`(c("Cadena","$x_1$", "$x_2$"))
```

```{r tabla numero efectivo de muestras rosenbrock 2}
#| tbl-cap: "Número efectivo de muestras para cadenas de 5000 muestras y distintos valores de $\\Sigma$"
#| tbl-pos: H

neff_tabla_ros |> 
  tt()
```

Considerando los tamaños de las muestras efectivas en cada dimensión y los gráficos de autocorrelación observados, parece que la matriz de covarianzas $\Sigma_2$ genera muestras que ajustan mejor a la distribución objetivo.


Para ver qué tan bien funciona Metropolis-Hastings en esta situación compleja, interesa calcular ciertas probabilidades y compararlas con las obtenidas aplicando el metodo de "Integración Numérica".


```{r probabilidades p10 1}
# i
pr_banana1 <- mean((cadena_p2$muestra_mh$dim_1 > 0 & cadena_p2$muestra_mh$dim_1 < 1) & 
       (cadena_p2$muestra_mh$dim_2 > 0 & cadena_p2$muestra_mh$dim_2 < 1)) |> round(4)

# ii
pr_banana2 <- mean((cadena_p2$muestra_mh$dim_1 > -1 & cadena_p2$muestra_mh$dim_1 < 0) & 
       (cadena_p2$muestra_mh$dim_2 > 0 & cadena_p2$muestra_mh$dim_2 < 1)) |> round(4)

# iii
pr_banana3 <- mean((cadena_p2$muestra_mh$dim_1 > 1 & cadena_p2$muestra_mh$dim_1 < 2) & 
       (cadena_p2$muestra_mh$dim_2 > 2 & cadena_p2$muestra_mh$dim_2 < 3)) |> round(4)
```

```{r probabilidades p10 2}
d_integral <- function(x, y) exp(-((0.5-x)^2 + 5 *(y-x^2)^2))

pr_valor_real1 <- integral2(d_integral, xmin = 0, xmax = 1, ymin = 0, ymax = 1)$Q |> round(4)

pr_valor_real2 <- integral2(d_integral, xmin = -1, xmax = 0, ymin = 0, ymax = 1)$Q |> round(4)

pr_valor_real3 <- integral2(d_integral, xmin = 1, xmax = 2, ymin = 2, ymax = 3)$Q |> round(4)

```

```{r tabla para probabilidad p10 1}
filas <- c("M-H", "Integración \\newline numérica")

probabilidades_banana <- tibble(
  var = filas,
  p1 = c(pr_banana1, pr_valor_real1),
  p2 = c(pr_banana2, pr_valor_real2),
  p3 = c(pr_banana3, pr_valor_real3)
) |> `colnames<-`(c("Método","$P(A, B)$", "$P(C, D)$", "$P(E, F)$"))
```

```{r tabla probabilidades banana 1}
#| tbl-cap: "Cálculo de probablidades con distintos métodos"

probabilidades_banana |> 
  tt(notes = list(
    "1" = list(i = 0, j = 2, text = "$A = 0 < X_1 < 1, B = 0 < X_2 < 1$"),
    "2" = list(i = 0, j = 3, text = "$C = -1 < X_1 < 0, D = 0 < X_2 < 1$"),
    "3" = list(i = 0, j = 4, text = "$E = 1 < X_1 < 2, F = 2 < X_2 < 3$")
  ))
  
```

Se puede observar que el algoritmo en esta situación falla bastante en la generación de las muestras, dado que la estimación de las probabilidades son muy diferentes a las calculadas por un método casi exacto.

# Conclusión

Metrópolis-Hastings es una herramienta muy útil y sencilla de emplear para generar muestras de distribuciones cuya complejidad se escapa de los métodos de muestreo más populares y directos. A lo largo del estudio se lograron detectar múltiples características tanto positivas como negativas de este método tan utilizado en el campo de la estadística bayesiana.

Este método brinda buenos resultados cuando la distribución a muestrear es unidimensional, o en el caso bidimensional siempre y cuando la distribución tenga una forma "sencilla", como por ejemplo una normal bivariada.

Una limitación que presenta es el costo computacional en casos de alta dimensionalidad, o cuando la forma de la distribución a muestrear es muy atípica como por ejemplo en el caso de la *banana de Rosenbrock*.

# Anexo

Se pueden replicar los resultados y comprobar los códigos utilizados consultando el [repositorio](https://github.com/GamboaNicolas/Metropolis-Hastings-examples.git) del trabajo.




