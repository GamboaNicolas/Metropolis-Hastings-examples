---
title: "Metropolis-Hasting"
format: 
  pdf:
    fig-pos: "H"
lang: es
echo: FALSE
message: FALSE
warning: FALSE
geometry:
  - top= 25mm
  - left= 20mm
  - right = 20mm
  - bottom = 25mm
  - heightrounded
header-includes:
  - \usepackage{ragged2e}
  - \usepackage{hyperref}

---


```{r Carga de librerias}
library(tidyverse)
library(mvtnorm)
library(patchwork)
library(tinytable)
library(pracma)
library(VGAM)
```

```{r Configuraciones predeterminadas}
knitr::opts_chunk$set(fig.align = "center", out.width = "70%")

set.seed("2126519")

theme_set(theme_bw())
```


# Introducción

Uno de los grandes problemas que existen en el mundo de la estadística es que hay distribuciones cuyas funciones de densidad son tan complejas que resultan difíciles de trabajar. Es por esto que muchas veces resulta de utilidad trabajar con muestras aleatorias, y a partir de estas responder determinadas preguntas. En la práctica, es común encontrarse con variables aleatorias con funciones de densidad cuyo muestreo directo no es simple.

Para estos casos existen distintas técnicas que nos permiten obtener muestras que, si bien no son tomadas de manera realmente independiente, se comportan de manera muy similar a como lo haría una muestra aleatoria independiente tomada de la función de densidad. 

Particularmente en el campo de la _Estadística Bayesiana_, esto resulta útil ya que permite obtener muestras de la distribución a posteriori, la cual se puede utilizar por ejemplo para obtener un intervalo de credibilidad del parámetro bajo estudio.

Un método sencillo de emplear y que reporta buenos resultados es el algoritmo de [_Metropolis-Hastings_](https://es.wikipedia.org/wiki/Algoritmo_de_Metropolis-Hastings). 

\newpage

# Metodología

En esta sección se presentan los algoritmo de metrópolis hasting para variables aleatorias unidimensionales y bidimensionales

(Agregar como funciona metropolis-hasting)

::: {.callout-note}

## Algoritmo Metrópolis-Hasting univariado

```{r algoritmo mh univariado doc, echo = T}
sample_mh <- function(n, d_objetivo, r_propuesta = NULL, d_propuesta = NULL, p_inicial = NULL){

  if (is.null(r_propuesta) | is.null(d_propuesta)) {
    r_propuesta <- function(media) rnorm(n = 1, media, sd = 1)
    d_propuesta <- function(x, media) dnorm(x = x,media, sd = 1)
  }
  
  stopifnot(n > 0)
  contador <- 0
  muestras <- numeric(n)
  
  muestras[1] <- p_inicial

  for(i in 2:n) {
    
    p_actual <- muestras[i-1]
    p_propuesta <- r_propuesta(p_actual)

    q_actual <- d_propuesta(p_actual, p_propuesta)
    q_nuevo <- d_propuesta(p_propuesta, p_actual)
    
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_propuesta)
    
    if (f_actual == 0 || q_nuevo == 0) {
      alfa <- 1
    } else {
      alfa <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    }

    muestras[i] <- sample(c(p_propuesta, p_actual),
    size = 1, prob = c(alfa, 1-alfa))
    
    if(muestras[i] != muestras[i-1]) {
      contador <- contador + 1
    }
  }
  return(list(cadena = data.frame(iteracion = 1:n, x = muestras), 
       tasa_aceptacion = contador / n))
}

```

:::

\newpage

::: {.callout-note}

## Algoritmo Metrópolis-Hasting bivariado

```{r algoritmo m-h bivariado doc, echo = T}
sample_mh_mv <- function(n, d_objetivo, cov_propuesta = diag(2), p_inicial = numeric(2)) {
  
  if (length(p_inicial) != 2) {
    stop("El valor p_inicial debe ser bidimensional")
  }
  if ( n <= 0 || n %% 1 != 0) {
    stop("El tamaño de muestra n debe ser entero y mayor que 0")
  }
  if (any((dim(cov_propuesta) != c(2,2)))) {
    stop("La matriz de covariancia debe ser de 2x2")
  }
  
  contador <- 0
  
  r_propuesta <-  function(media) rmvnorm(n = 1,mean = media,sigma = cov_propuesta)
  d_propuesta <- function(x, media) dmvnorm(x = x,mean = media,sigma = cov_propuesta)
  
  muestras <- matrix(0,nrow = n,ncol = length(p_inicial))
  muestras[1, ] <- p_inicial
  
  for(i in 2:n) {
    p_actual <- muestras[i-1,]
    p_propuesta <- r_propuesta(p_actual)
    
    q_actual <- d_propuesta(p_actual, p_propuesta)
    q_nuevo <- d_propuesta(p_propuesta, p_actual)
    
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_propuesta)
    
    
    if (f_actual == 0 || q_nuevo == 0) {
      alfa <- 1
    } else {
      
      alfa <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    }
    
    aceptar <- rbinom(1,1,alfa)
    
    if (aceptar) {
      muestras[i,] <- p_propuesta
    }else{
      muestras[i,] <- p_actual
    }
    
    if(!any(muestras[i,] != muestras[i-1,])) {
      contador <- contador + 1
    }
  }
  salida <- data.frame(iteracion = 1:n, x = muestras)
  colnames(salida) <- c("iteracion", paste0("dim_",1:length(p_inicial)))
  
  
  return(list(muestra_mh = salida,
       probabilidad_aceptacion = contador / n)) # REVISAR -----------

}
```

:::

# Discusiones

## Distribución de Kumaraswamy

La distribución de Kumaraswamy es una distribución de probabilidad continua que se utiliza para modelar variables aleatorias con soporte en el intervalo $(0,1)$, cuya función de densidad es: $f(x|a,b) = abx^{a-1}(1-x^a)^{b-1}, \ con \ a,b>0$

```{r distribucion de kumaraswamy}
#| fig-cap: "Función de densidad de la distribución de Kumaraswamy"

grilla <- seq(0, 1, length.out = 502)[c(-1,-502)]
d_kuma <- function(x, a = 6, b = 2){
  
  if(a<0 | b<0) stop("Poneme a y b positivos no te cuesta nada")
  
  salida <- a*b*x^(a-1)*(1-x^a)^(b-1)
  
  salida[which(x <= 0 | x >= 1)] <- 0
  
  salida
}



tibble(
  x = rep(grilla,times = 5),
  Parámetros = rep(c("a = 0.5, b = 0.5",
                     "a = 1, b = 2",
                     "a = 3, b = 4",
                     "a = 6, b = 2",
                     "a = 5, b = 1"), each = 500),
  valores = c(
    d_kuma(grilla, 0.5, 0.5),
    d_kuma(grilla, 1, 2),
    d_kuma(grilla, 3, 4),
    d_kuma(grilla, 6, 2),
    d_kuma(grilla, 5,1)
  )
) |> 
  ggplot()+
  aes(x = x, y = valores,color = Parámetros)+
  geom_line(linewidth = 1.2) +
  labs(x = "X", y = expression(f ~ "("~x~" | a, b)"))

# tibble(
#   x = rep(grilla,times = 5),
#   parametros = rep(c("a = 0.5, b = 0.5",
#                      "a = 1, b = 2",
#                      "a = 3, b = 4",
#                      "a = 6, b = 2",
#                      "a = 5, b = 1"), each = 500),
#   valores = c(
#     dbeta(grilla, 0.5, 0.5),
#     dbeta(grilla, 1, 2),
#     dbeta(grilla, 3, 4),
#     dbeta(grilla, 6, 2),
#     dbeta(grilla, 5,1)
#   )
# ) |> 
#   ggplot()+
#   aes(x = x, y = valores,color = parametros)+
#   geom_line(linewidth = 1.2)
# 
```

Esta distribución puede ser utilizada en el ámbito de la estadística bayesiana a la hora de definir un prior para un parámetro con campo de variación en el intervalo (0, 1).

Por lo general la distribución elegida para estas situaciones suele ser la beta ya que presenta ventajas como ser una distribución conjugada de la binomial, lo cual puede facilitar mucho algunos cálculos. El problema es que la densidad de esta depende de la función gamma, la cual es una integral, y en algunas situaciones se puede complicar su cálculo.

La distribución de Kumaraswamy se comporta de manera muy similar a la beta, sin tener el problema de la dificultad del cálculo de la integral. 

## Metropolis-Hasting en una dimensión

A continuación utilizaremos el algoritmo de Metropolis-Hasting para generar 5000 muestras de la distribución de _Kumaraswamy_ con parámetros $a = 6$ y $b = 2$, utilizando como distribución propuesta una $Beta(\mu, \kappa)$, donde $\mu$ representa la media de la distribución y $\kappa$ el grado de la concentración de la distribución. Esto lo haremos para 3 valores distintos de $\kappa$.

```{r Algoritmo M-H univariado}
# sample_mh <- function(n,
#                       d_objetivo,
#                       r_propuesta = NULL,
#                       d_propuesta = NULL,
#                       p_inicial = NULL){
# 
#   if (is.null(r_propuesta) | is.null(d_propuesta)) {
#     r_propuesta <- function(media) rnorm(n = 1, media, sd = 1)
#     d_propuesta <- function(x, media) dnorm(x = x,media, sd = 1)
#   }
# 
#   stopifnot(n > 0)
#   contador <- 0
#   muestras <- numeric(n)
# 
#   # if (is.null(p_inicial)) {
#   #   for(i in 1:1000){
#   #     p_inicial <- runif(1,-100,100)
#   #     if (d_objetivo(p_inicial) != 0) {
#   #       break
#   #     }
#   #   }
#   #   if(i == 1000) stop("Se recomienda introducir un valor para ´p_inicial´")
#   # }
# 
#   muestras[1] <- p_inicial
# 
#   for(i in 2:n) {
# 
#     p_actual <- muestras[i-1]
#     p_propuesta <- r_propuesta(p_actual)
# 
#     q_actual <- d_propuesta(p_actual, p_propuesta)
#     q_nuevo <- d_propuesta(p_propuesta, p_actual)
# 
#     f_actual <- d_objetivo(p_actual)
#     f_nuevo <- d_objetivo(p_propuesta)
# 
#     if (f_actual == 0 || q_nuevo == 0) {
#       alfa <- 1
#     } else {
# 
#       alfa <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
#     }
# 
# 
#     muestras[i] <- sample(c(p_propuesta, p_actual),
#     size = 1, prob = c(alfa, 1-alfa))
# 
#     if(muestras[i] != muestras[i-1]) {
#       contador <- contador + 1
#     }
#   }
#   return(list(cadena = data.frame(iteracion = 1:n, x = muestras),
#        tasa_aceptacion = contador / n))
# }
# 
# 
# # d_obj <- function(x){
# #   # dnorm(x = x,mean = 90,sd = 8)
# #   return(dbeta(x = x,2,2))
# # }
# # d_obj(-50)
# # muestra <- sample_mh(100000, d_objetivo = d_obj)
# #
# # muestra |>
# #   ggplot()+
# #   aes(x = x)+
# #   geom_histogram()
```

```{r comparacion de las cadenas}
source("Funciones.R")

get_beta_param <- function(media, kappa) {
  alpha <- media*kappa
  beta <- (1-media)*kappa
  
  return(list(alpha = alpha, beta = beta))
}
```


```{r cadenas markov, include=F}
valores_kappa <- c(1, 2, 5)

graficos <- list(Kappa1 = NULL, Kappa2 = NULL, Kappa5 = NULL)

df_cadenas <- list(data_cadena_1 = NULL, data_cadena_2 = NULL, data_cadena_3 = NULL)

for(i in 1:3) {
  r_prop <- function(x) {
  kappa <- valores_kappa[i]
  pars <- get_beta_param(x, kappa)
  rbeta(1, shape1 = pars$alpha, shape2 = pars$beta)
}
d_prop <- function(x, mean) {
  kappa <- valores_kappa[i]
  pars <- get_beta_param(mean, kappa)
  dbeta(x, shape1 = pars$alpha, shape2 = pars$beta)
}

muestra <- sample_mh(n = 5000, d_objetivo = d_kuma, r_propuesta = r_prop, d_propuesta = d_prop, p_inicial = rbeta(1,2,2))

ap1 <- plot_autocor(muestra = muestra$cadena)

tp1 <- plot_trace(muestra = muestra$cadena)

hp1 <- plot_hist(muestra = muestra$cadena, d_objetivo = d_kuma)

graficos[[i]] <- (tp1 / (hp1 + ap1))

df_cadenas[[i]] <- muestra
}

neff1 <- n_eff(df_cadenas$data_cadena_1$cadena[2])
neff2 <- n_eff(df_cadenas$data_cadena_2$cadena[2])
neff3 <- n_eff(df_cadenas$data_cadena_3$cadena[2])

```


```{r cadena 1}
#| fig-cap: "Muestreo por Metropolis-Hastings con $\\kappa$ = 1"

graficos$Kappa1
```


```{r cadena 2}
#| fig-cap: "Muestreo por Metropolis-Hastings con $\\kappa$ = 2"

graficos$Kappa2
```


```{r cadena 3}
#| fig-cap: "Muestreo por Metropolis-Hastings con $\\kappa$ = 5"

graficos$Kappa5
```

Teniendo en cuenta que estas muestras son dependientes resulta de interés conocer a cuantas muestras independientes equivalen, dado que cuanto más se comporta una cadena de Markov dependiente como una muestra independiente, menor es el error en la aproximación posterior resultante. Para ello se calcula el número efectivo de muestras $(n_{eff})$ para cada cadena.

```{r tabla nro efectivo de muestra}
cadenas_text <- c("Cadena 1", "Cadena 2", "Cadena 3")

nro_ef_muestras <- tibble(
  Cadena = cadenas_text,
  kappa = rep(c(1,2,5)),
  n_eff = floor(c(neff1,neff2,neff3))
) |> `colnames<-`(c("Cadena", "$\\kappa$", "$n_{eff}$"))
```


```{r tabla nro efectivo de muestra 2}
#| tbl-cap: "Numero efectivo de muestras para cadenas de 5000 muestras y distintos valores de $\\kappa$"

nro_ef_muestras |> 
  tt() 
```

Con esto concluimos que el valor de $\kappa$ que arroja una muestra relacionada que equivale a una independiente de mayor tamaño es 5. Sin embargo esta cantidad de muestras independientes es muy pobre teniendo en cuenta que el tamaño de la cadena es de 5000 muestras.


```{r funcion x}

# Creación de la tabla

# Cadena 1
media_cadena_1 <- mean(df_cadenas$data_cadena_1$cadena$x) |> round(3)
percentiles_1 <- quantile(df_cadenas$data_cadena_1$cadena$x, c(0.05,0.95)) |> round(3)

# Cadena 2
media_cadena_2 <- mean(df_cadenas$data_cadena_2$cadena$x) |> round(3)
percentiles_2 <- quantile(df_cadenas$data_cadena_2$cadena$x, c(0.05,0.95)) |> round(3)

# Cadena 3
media_cadena_3 <- mean(df_cadenas$data_cadena_3$cadena$x) |> round(3)
percentiles_3 <- quantile(df_cadenas$data_cadena_3$cadena$x, c(0.05,0.95)) |> round(3)

# Media y cuantiles verdaderos

g <- function(x) x*d_kuma(x, 6, 2)
media_kumar <- integrate(g, 0, 1)$value |> round(digits = 3)

kumar_05 <- qkumar(p = 0.05, shape1 = 6, shape2 = 2) |> round(digits = 3)
kumar_95 <- qkumar(p = 0.95, shape1 = 6, shape2 = 2) |> round(digits = 3)

logit <- function(x) log(x/(1-x))

# Cadena 1
media_cadena_lg_1 <- mean(logit(df_cadenas$data_cadena_1$cadena$x)) |> round(3)
percentiles_lg_1 <- quantile(logit(df_cadenas$data_cadena_1$cadena$x), c(0.05,0.95)) |> round(3)

# Cadena 2
media_cadena_lg_2 <- mean(logit(df_cadenas$data_cadena_2$cadena$x)) |> round(3)
percentiles_lg_2 <- quantile(logit(df_cadenas$data_cadena_2$cadena$x), c(0.05,0.95)) |> round(3)

# Cadena 3
media_cadena_lg_3 <- mean(logit(df_cadenas$data_cadena_3$cadena$x)) |> round(3)
percentiles_lg_3 <- quantile(logit(df_cadenas$data_cadena_3$cadena$x), c(0.05,0.95)) |> round(3)

# Media y cuantiles verdaderos
g_lg <- function(x) logit(x)*(d_kuma(x, 6, 2))

media_kumar_lg <- integrate(g_lg, 0, 1)$value |> round(digits = 3)
kumar_05_lg <- logit(qkumar(p = 0.05, shape1 = 6, shape2 = 2)) |> round(digits = 3)
kumar_95_lg <- logit(qkumar(p = 0.95, shape1 = 6, shape2 = 2)) |> round(digits = 3)

```

\newpage


Resulta de interés ver como afecta la elección del parámetro $\kappa$ al utilizar Metropolis-Hasting para obtener muestras de la distribución de Kumaraswamy usando una distribución $Beta$ como propuesta. Es por esto que se decide obtener la media y ciertos cuantiles sobre las muestras obtenidas y sobre la función Logit de estas.

```{r tablita fachera}
filas <- rep(c("$X$", "$Logit(X)$"), each = 3)

valores <- tibble(
  var = filas,
  kappa = rep(c("1","2","5"), times = 2), 
  esp_est = c(media_cadena_1, media_cadena_2, media_cadena_3, media_cadena_lg_1, media_cadena_lg_2, media_cadena_lg_3),
  quantil_0.05 = c(percentiles_1[1], percentiles_2[1], percentiles_3[1], percentiles_lg_1[1], percentiles_lg_2[1], percentiles_lg_3[1]),
  quantil_0.95 = c(percentiles_1[2], percentiles_2[2], percentiles_3[2], percentiles_lg_1[2], percentiles_lg_2[2], percentiles_lg_3[2])
) |> 
  `colnames<-`(c("$f(x)$" ,"$\\kappa$", "$\\hat{E(x)}$", "$\\hat{q_{0.05}}$", "$\\hat{q_{0.95}}$"))
```


```{r tablita fachera 2}
#| tbl-cap: "Media y percentiles estimadas para las funciones $X$ y $Logit(X)$"

valores |> 
  tt() |> 
  style_tt(i = c(1,4), j = 1, rowspan = 3, alignv = "t")
```


Las estadísticas reales de la distribución de Kumaraswamy son:

```{r valores reales kuma}
filas <- c("$X$", "$Logit(X)$")

valores_reales <- tibble(
  var = filas,
  esp_est = c(media_kumar,media_kumar_lg),
  quantil_0.05 = c(kumar_05,kumar_05_lg),
  quantil_0.95 = c(kumar_95,kumar_95_lg)
) |> 
  `colnames<-`(c("$f(x)$" ,"$E(x)$", "$q_{0.05}$", "$q_{0.95}$"))
```


```{r valores reales kuma 2}
#| tbl-cap: "Media y percentiles para las funciones $X$ y $Logit(X)$"

valores_reales |> 
  tt()
```

Si bien las diferencias en dispersión de las muestras no son muy perceptibles a simple vista, gracias a la funcióm logit podemos percibirlas con mayor facilidad. Es así que se puede concluir que un $\kappa$

# Metropolis-Hasting en dos dimensiónes





:::callout-note

```{r}
sample_mh_mv <- function(n, d_objetivo, cov_propuesta = diag(2), p_inicial = numeric(2)) {
  
  if (length(p_inicial) != 2) {
    stop("El valor p_inicial debe ser bidimensional")
  }
  if ( n <= 0 || n %% 1 != 0) {
    stop("El tamaño de muestra n debe ser entero y mayor que 0")
  }
  if (any((dim(cov_propuesta) != c(2,2)))) {
    stop("La matriz de covariancia debe ser de 2x2")
  }
  
  contador <- 0
  
  r_propuesta <-  function(media) rmvnorm(n = 1,mean = media,sigma = cov_propuesta)
  d_propuesta <- function(x, media) dmvnorm(x = x,mean = media,sigma = cov_propuesta)
  
  muestras <- matrix(0,nrow = n,ncol = length(p_inicial))
  muestras[1, ] <- p_inicial
  
  for(i in 2:n) {
    p_actual <- muestras[i-1,]
    p_propuesta <- r_propuesta(p_actual)
    
    q_actual <- d_propuesta(p_actual, p_propuesta)
    q_nuevo <- d_propuesta(p_propuesta, p_actual)
    
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_propuesta)
    
    
    if (f_actual == 0 || q_nuevo == 0) {
      alfa <- 1
    } else {
      
      alfa <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    }
    
    aceptar <- rbinom(1,1,alfa)
    
    if (aceptar) {
      muestras[i,] <- p_propuesta
    }else{
      muestras[i,] <- p_actual
    }
    
    if(!any(muestras[i,] != muestras[i-1,])) {
      contador <- contador + 1
    }
  }
  salida <- data.frame(iteracion = 1:n, x = muestras)
  colnames(salida) <- c("iteracion", paste0("dim_",1:length(p_inicial)))
  
  
  return(list(muestra_mh = salida,
       probabilidad_aceptacion = contador / n)) # REVISAR -----------

}
```
:::


```{r}
mu <- c(.4,.75) 
sigma <- matrix(c(1.35,.4,.4,2.4),2)
d_obj <- function(x) dmvnorm(x,mean = mu,sigma = sigma)

cov_prop <- diag(2)

cadena <- sample_mh_mv(n = 10000,d_objetivo = d_obj, cov_propuesta = cov_prop, p_inicial = c(0,0))

```



```{r}
#| fig-cap: "Titulos"
#| fig-subcap: "lallala"
plot_hotmap(muestra = cadena$muestra_mh, d_objetivo = d_obj, puntos = F)

```

```{r}
plot_trace(muestra = cadena$muestra_mh)
```

Probabilidades
```{r}
# i
mean(cadena$muestra_mh$dim_1 > 1 & cadena$muestra_mh$dim_2 < 0)

# ii
mean(cadena$muestra_mh$dim_1 > 1 & cadena$muestra_mh$dim_2 > 2)

# iii
mean(cadena$muestra_mh$dim_1 > 0.4 & cadena$muestra_mh$dim_2 > 0.75)

```


```{r}

prob_1 <- pmvnorm(lower = c(1, -Inf), upper = c(Inf, 0), mean = mu, sigma = sigma)

prob_2 <- pmvnorm(lower = c(1, 2), upper = c(Inf, Inf), mean = mu, sigma = sigma)

prob_3 <- pmvnorm(lower = c(0.4, 0.75), upper = c(Inf, Inf), mean = mu, sigma = sigma)


as.numeric(prob_1)
as.numeric(prob_2)
as.numeric(prob_3)

```

```{r}
d_obj_p <- function(x, a = 0.5, b = 5) exp(-((a-x[1])^2 + b *(x[2]-x[1]^2)^2))

cov_prop_p <- diag(c(0.06, 0.07))

cadena_p <- sample_mh_mv(n = 5000, d_objetivo = d_obj_p, cov_propuesta = cov_prop_p, p_inicial = c(0,0))

cadena_p$probabilidad_aceptacion

acf(cadena_p$muestra_mh[-1], plot = F) # Decorar

plot_trace(cadena_p$muestra_mh)

plot_hotmap(cadena_p$muestra_mh[-1], d_objetivo = d_obj_p, puntos = F)


```


```{r}
cov_prop_p <- diag(c(1, 1))

cadena_p <- sample_mh_mv(n = 5000, d_objetivo = d_obj_p, cov_propuesta = cov_prop_p, p_inicial = c(0,0))

cadena_p$probabilidad_aceptacion

acf(cadena_p$muestra_mh[-1], plot = F) # Decorar

plot_trace(cadena_p$muestra_mh)

plot_hotmap(cadena_p$muestra_mh[-1], d_objetivo = d_obj_p, puntos = F)


```


```{r}
cov_prop_p <- diag(c(0.5, 0.5))

cadena_p <- sample_mh_mv(n = 5000, d_objetivo = d_obj_p, cov_propuesta = cov_prop_p, p_inicial = c(0,0))

cadena_p$probabilidad_aceptacion


acf(cadena_p$muestra_mh[-1], plot = F) # Decorar

plot_trace(cadena_p$muestra_mh)

plot_hotmap(cadena_p$muestra_mh[-1], d_objetivo = d_obj_p, puntos = F)

```


```{r}
# i
mean((cadena_p$muestra_mh$dim_1 > 0 & cadena_p$muestra_mh$dim_1 < 1) & 
       (cadena_p$muestra_mh$dim_2 > 0 & cadena_p$muestra_mh$dim_2 < 1))

# ii
mean((cadena_p$muestra_mh$dim_1 > -1 & cadena_p$muestra_mh$dim_1 < 0) & 
       (cadena_p$muestra_mh$dim_2 > 0 & cadena_p$muestra_mh$dim_2 < 1))

# iii
mean((cadena_p$muestra_mh$dim_1 > 1 & cadena_p$muestra_mh$dim_1 < 2) & 
       (cadena_p$muestra_mh$dim_2 > 2 & cadena_p$muestra_mh$dim_2 < 3))
```

```{r}
d_integral <- function(x, y) exp(-((0.5-x)^2 + 5 *(y-x^2)^2))

integral2(d_integral, xmin = 0, xmax = 1, ymin = 0, ymax = 1)$Q

integral2(d_integral, xmin = -1, xmax = 0, ymin = 0, ymax = 1)$Q

integral2(d_integral, xmin = 1, xmax = 2, ymin = 2, ymax = 3)$Q

```


# Preguntas y propuestas

Comparar las probabilidades tmb por monte carlo
Preguntar por la probabilidad de salto en mh_multivariado
Como es el numerador del NEFF (Se calcula con una o varias cadenas?)