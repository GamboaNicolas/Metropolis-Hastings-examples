---
title: "Metropolis-Hasting"
format: 
  pdf:
    fig-pos: "H"
lang: es
echo: FALSE
message: FALSE
warning: FALSE
geometry:
  - top= 25mm
  - left= 20mm
  - right = 20mm
  - bottom = 25mm
  - heightrounded
header-includes:
  - \usepackage{ragged2e}
  - \usepackage{hyperref}

---


```{r Carga de librerias}
library(tidyverse)
library(mvtnorm)
library(patchwork)
library(tinytable)
```

```{r Configuraciones predeterminadas}
knitr::opts_chunk$set(fig.align = "center", out.width = "70%")

set.seed("2126519")

theme_set(theme_bw())
```


# Introducción

Uno de los grandes problemas en el mundo de la estadistica es que existen distribuciones cuyas funciones de densidad son tan complejas que es difil de trabajar con estas, es por esto que muchas veces resulta de utilidad trabajar con muestras aleatorias de estas variables, y a partir de estas muestras resppnder determinadas preguntas. En la práctica, es comun encontrarse con variables aleatorias con función de densidad cuyo muestreo directo no es simple. 

Para estos casos existen distintas técnicas que nos permiten obtener muestras que, si bien no son tomadas de manera realmente independiente, se comportan de manera muy similar a como lo haría una muestra aleatoria independiente tomada de la función de densidad. Un método sencillo de emplear y que reporta buenos resultados es Metropolis-Hastings. 

Cuando n tiende al infinito asegura convergencia????

, es por esto que el algortmo de Metropolis-Hastings es de tanta utilidad

En el campo de la estadistica bayesiana esto es especialmente util para obtener muestras de un parametro cuya funcion de densidad a posteriori es compleja.

Uno de los grandes problemas en el mundo de la estadistica es extraer muestras de funciones de densidad complejas que es difil de trabajar con estas 

ES importante en la estadistica bayesiana poder extraer muestras de nuestro parametro estimado, pero que pasa cuando la distribucion a posteriori de este es compleja

# Metropolis-Hasting en una dimensión
  
A continuación se presenta...


:::callout-note
## Algoritmo M-H univariado

```{r Algoritmo M-H univariado}
sample_mh <- function(n, 
                      d_objetivo, 
                      r_propuesta = NULL,
                      d_propuesta = NULL,
                      p_inicial = NULL){

  if (is.null(r_propuesta) | is.null(d_propuesta)) {
    r_propuesta <- function(media) rnorm(n = 1, media, sd = 1)
    d_propuesta <- function(x, media) dnorm(x = x,media, sd = 1)
  }
  
  stopifnot(n > 0)
  contador <- 0
  muestras <- numeric(n)
  
  # if (is.null(p_inicial)) {
  #   for(i in 1:1000){
  #     p_inicial <- runif(1,-100,100)
  #     if (d_objetivo(p_inicial) != 0) {
  #       break
  #     }
  #   }
  #   if(i == 1000) stop("Se recomienda introducir un valor para ´p_inicial´")
  # }
  
  muestras[1] <- p_inicial

  for(i in 2:n) {
    
    p_actual <- muestras[i-1]
    p_propuesta <- r_propuesta(p_actual)

    q_actual <- d_propuesta(p_actual, p_propuesta)
    q_nuevo <- d_propuesta(p_propuesta, p_actual)
    
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_propuesta)
    
    if (f_actual == 0 || q_nuevo == 0) {
      alfa <- 1
    } else {
      
      alfa <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    }
    

    muestras[i] <- sample(c(p_propuesta, p_actual),
    size = 1, prob = c(alfa, 1-alfa))
    
    if(muestras[i] != muestras[i-1]) {
      contador <- contador + 1
    }
  }
  return(list(cadena = data.frame(iteracion = 1:n, x = muestras), 
       tasa_aceptacion = contador / n))
}


# d_obj <- function(x){
#   # dnorm(x = x,mean = 90,sd = 8)
#   return(dbeta(x = x,2,2))
# }
# d_obj(-50)
# muestra <- sample_mh(100000, d_objetivo = d_obj)
# 
# muestra |> 
#   ggplot()+
#   aes(x = x)+
#   geom_histogram()
```
:::


# Distribución de Kumaraswamy




```{r distribucion de kumaraswamy}
grilla <- seq(0, 1, length.out = 502)[c(-1,-502)]
d_kuma <- function(x, a = 6, b = 2){
  
  if(a<0 | b<0) stop("Poneme a y b positivos no te cuesta nada")
  
  salida <- a*b*x^(a-1)*(1-x^a)^(b-1)
  
  salida[which(x <= 0 | x >= 1)] <- 0
  
  salida
}



tibble(
  x = rep(grilla,times = 5),
  parametros = rep(c("a = 0.5, b = 0.5",
                     "a = 1, b = 2",
                     "a = 3, b = 4",
                     "a = 6, b = 2",
                     "a = 5, b = 1"), each = 500),
  valores = c(
    d_kuma(grilla, 0.5, 0.5),
    d_kuma(grilla, 1, 2),
    d_kuma(grilla, 3, 4),
    d_kuma(grilla, 6, 2),
    d_kuma(grilla, 5,1)
  )
) |> 
  ggplot()+
  aes(x = x, y = valores,color = parametros)+
  geom_line(linewidth = 1.2)

# tibble(
#   x = rep(grilla,times = 5),
#   parametros = rep(c("a = 0.5, b = 0.5",
#                      "a = 1, b = 2",
#                      "a = 3, b = 4",
#                      "a = 6, b = 2",
#                      "a = 5, b = 1"), each = 500),
#   valores = c(
#     dbeta(grilla, 0.5, 0.5),
#     dbeta(grilla, 1, 2),
#     dbeta(grilla, 3, 4),
#     dbeta(grilla, 6, 2),
#     dbeta(grilla, 5,1)
#   )
# ) |> 
#   ggplot()+
#   aes(x = x, y = valores,color = parametros)+
#   geom_line(linewidth = 1.2)
# 

```


Esta distribución puede ser utilizada en el ámbito de la estadística bayesiana a la hora de definir un prior para un parámetro con campo de variación en el intervalo (0, 1).

Por lo general la distribución elegida para estas situaciones suele ser la beta ya que presenta ventajas como ser una distribución conjugada de la binomial, lo cual puede facilitar mucho algunos cálculos. El problema es que para calcular la densidad de esta es que depende de la función gamma, la cual es una integral, y en algunas situaciones se puede complicar su cálculo.

La distribución de Kumaraswamy se comporta de manera muy similar a la beta, sin tener el problema de la dificultad del cálculo de la densidad. 


```{r comparacion de las cadenas}
source("Funciones.R")

get_beta_param <- function(media, kappa) {
  alpha <- media*kappa
  beta <- (1-media)*kappa
  
  return(list(alpha = alpha, beta = beta))
}
```


```{r cadenas markov, include=F}
valores_kappa <- c(1, 2, 5)

graficos <- list(Kappa1 = NULL, Kappa2 = NULL, Kappa5 = NULL)

df_cadenas <- list(data_cadena_1 = NULL, data_cadena_2 = NULL, data_cadena_3 = NULL)

for(i in 1:3) {
  r_prop <- function(x) {
  kappa <- valores_kappa[i]
  pars <- get_beta_param(x, kappa)
  rbeta(1, shape1 = pars$alpha, shape2 = pars$beta)
}
d_prop <- function(x, mean) {
  kappa <- valores_kappa[i]
  pars <- get_beta_param(mean, kappa)
  dbeta(x, shape1 = pars$alpha, shape2 = pars$beta)
}

muestra <- sample_mh(n = 5000, d_objetivo = d_kuma, r_propuesta = r_prop, d_propuesta = d_prop, p_inicial = rbeta(1,2,2))

ap1 <- plot_autocor(muestra = muestra$cadena)

tp1 <- plot_trace(muestra = muestra$cadena)

hp1 <- plot_hist(muestra = muestra$cadena, d_objetivo = d_kuma)

graficos[[i]] <- (tp1 / (hp1 + ap1))

df_cadenas[[i]] <- muestra
}
```


```{r cadena 1}
#| fig-cap: "Muestreo por Metropolis-Hastings con concentación de 1"

graficos$Kappa1
```


```{r cadena 2}
#| fig-cap: "Muestreo por Metropolis-Hastings con concentación de 2"

graficos$Kappa2
```


```{r cadena 3}
#| fig-cap: "Muestreo por Metropolis-Hastings con concentación de 5"

graficos$Kappa5
```

```{r funcion x}
# Cadena 1
media_cadena_1 <- mean(df_cadenas$data_cadena_1$cadena$x) |> round(3)
percentiles_1 <- quantile(df_cadenas$data_cadena_1$cadena$x, c(0.05,0.95)) |> round(3)

# Cadena 2
media_cadena_2 <- mean(df_cadenas$data_cadena_2$cadena$x) |> round(3)
percentiles_2 <- quantile(df_cadenas$data_cadena_2$cadena$x, c(0.05,0.95)) |> round(3)

# Cadena 3
media_cadena_3 <- mean(df_cadenas$data_cadena_3$cadena$x) |> round(3)
percentiles_3 <- quantile(df_cadenas$data_cadena_3$cadena$x, c(0.05,0.95)) |> round(3)

```

```{r funcion logit}
logit <- function(x) log(x/(1-x))

# Cadena 1
media_cadena_lg_1 <- mean(logit(df_cadenas$data_cadena_1$cadena$x)) |> round(3)
percentiles_lg_1 <- quantile(logit(df_cadenas$data_cadena_1$cadena$x), c(0.05,0.95)) |> round(3)

# Cadena 2
media_cadena_lg_2 <- mean(logit(df_cadenas$data_cadena_2$cadena$x)) |> round(3)
percentiles_lg_2 <- quantile(logit(df_cadenas$data_cadena_2$cadena$x), c(0.05,0.95)) |> round(3)

# Cadena 3
media_cadena_lg_3 <- mean(logit(df_cadenas$data_cadena_3$cadena$x)) |> round(3)
percentiles_lg_3 <- quantile(logit(df_cadenas$data_cadena_3$cadena$x), c(0.05,0.95)) |> round(3)

```


```{r tablita fachera}
filas <- rep(c("$X$", "$Logit(X)$"), each = 3)
columnas <- c("Kappa", "Esperanza_estimada", "Quantil_0.05", "Quantil_0.95")

valores <- tibble(
  var = filas,
  kappa = rep(c(1,2,5), times = 2), 
  esp_est = c(media_cadena_1, media_cadena_2, media_cadena_3, media_cadena_lg_1, media_cadena_lg_2, media_cadena_lg_3),
  quantil_0.05 = c(percentiles_1[1], percentiles_2[1], percentiles_3[1], percentiles_lg_1[1], percentiles_lg_2[1], percentiles_lg_3[1]),
  quantil_0.95 = c(percentiles_1[2], percentiles_2[2], percentiles_3[2], percentiles_lg_1[2], percentiles_lg_2[2], percentiles_lg_3[2])
)

colnames(valores) <- c("$f(x)$" ,"$\\kappa$", "$\\hat{E(x)}$", "$\\hat{q_{0.05}}$", "$\\hat{q_{0.95}}$")

tablita <- valores |> 
  tt() |> 
  style_tt(i = c(1,4), j = 1, rowspan = 3, alignv = "t")
  # style_tt(i = c(1,4), j = c(2:5),background = "pink")
tablita
```



# Metropolis-Hasting en dos dimensiónes





:::callout-note

```{r}
sample_mh_mv <- function(n, d_objetivo, cov_propuesta = diag(2), p_inicial = numeric(2)) {
  
  if (length(p_inicial) != 2) {
    stop("El valor p_inicial debe ser bidimensional")
  }
  if ( n <= 0 || n %% 1 != 0) {
    stop("El tamaño de muestra n debe ser entero y mayor que 0")
  }
  if (any((dim(cov_propuesta) != c(2,2)))) {
    stop("La matriz de covariancia debe ser de 2x2")
  }
  
  contador <- 0
  
  r_propuesta <-  function(media) rmvnorm(n = 1,mean = media,sigma = cov_propuesta)
  d_propuesta <- function(x, media) dmvnorm(x = x,mean = media,sigma = cov_propuesta)
  
  muestras <- matrix(0,nrow = n,ncol = length(p_inicial))
  muestras[1, ] <- p_inicial
  
  for(i in 2:n) {
    p_actual <- muestras[i-1,]
    p_propuesta <- r_propuesta(p_actual)
    
    q_actual <- d_propuesta(p_actual, p_propuesta)
    q_nuevo <- d_propuesta(p_propuesta, p_actual)
    
    f_actual <- d_objetivo(p_actual)
    f_nuevo <- d_objetivo(p_propuesta)
    
    
    if (f_actual == 0 || q_nuevo == 0) {
      alfa <- 1
    } else {
      
      alfa <- min(1, (f_nuevo/f_actual)*(q_actual/q_nuevo))
    }
    
    aceptar <- rbinom(1,1,alfa)
    
    if (aceptar) {
      muestras[i,] <- p_propuesta
    }else{
      muestras[i,] <- p_actual
    }
    
    if(!any(muestras[i,] != muestras[i-1,])) {
      contador <- contador + 1
    }
  }
  salida <- data.frame(iteracion = 1:n, x = muestras)
  colnames(salida) <- c("iteracion", paste0("dim_",1:length(p_inicial)))
  
  
  return(list(muestra_mh = salida,
       probabilidad_aceptacion = contador / n)) # REVISAR -----------

}
```
:::


```{r}
mu <- c(.4,.75) 
sigma <- matrix(c(1.35,.4,.4,2.4),2)
d_obj <- function(x) dmvnorm(x,mean = mu,sigma = sigma)

cov_prop <- diag(2)

cadena <- sample_mh_mv(n = 10000,d_objetivo = d_obj, cov_propuesta = cov_prop, p_inicial = c(0,0))

```



```{r}
#| fig-cap: "Titulos"
#| fig-subcap: "lallala"
plot_hotmap(muestra = cadena$muestra_mh, d_objetivo = d_obj, puntos = F)

```

```{r}
plot_trace(muestra = cadena$muestra_mh)
```

Probabilidades
```{r}
# i
mean(cadena$muestra_mh$dim_1 > 1 & cadena$muestra_mh$dim_2 < 0)

# ii
mean(cadena$muestra_mh$dim_1 > 1 & cadena$muestra_mh$dim_2 > 2)

# iii
mean(cadena$muestra_mh$dim_1 > 0.4 & cadena$muestra_mh$dim_2 > 0.75)

```


```{r}

prob_1 <- pmvnorm(lower = c(1, -Inf), upper = c(Inf, 0), mean = mu, sigma = sigma)

prob_2 <- pmvnorm(lower = c(1, 2), upper = c(Inf, Inf), mean = mu, sigma = sigma)

prob_3 <- pmvnorm(lower = c(0.4, 0.75), upper = c(Inf, Inf), mean = mu, sigma = sigma)


as.numeric(prob_1)
as.numeric(prob_2)
as.numeric(prob_3)

```

```{r}
d_obj_p <- function(x, a = 0.5, b = 5) exp(-((a-x[1])^2 + b *(x[2]-x[1]^2)^2))

cov_prop_p <- diag(c(0.06, 0.07))

cadena_p <- sample_mh_mv(n = 5000, d_objetivo = d_obj_p, cov_propuesta = cov_prop_p, p_inicial = c(0,0))

cadena_p$probabilidad_aceptacion

acf(cadena_p$muestra_mh[-1]) # Decorar

plot_trace(cadena_p$muestra_mh)

plot_hotmap(cadena_p$muestra_mh[-1], d_objetivo = d_obj_p, puntos = F)


```


```{r}
cov_prop_p <- diag(c(1, 1))

cadena_p <- sample_mh_mv(n = 5000, d_objetivo = d_obj_p, cov_propuesta = cov_prop_p, p_inicial = c(0,0))

cadena_p$probabilidad_aceptacion

acf(cadena_p$muestra_mh[-1]) # Decorar

plot_trace(cadena_p$muestra_mh)

plot_hotmap(cadena_p$muestra_mh[-1], d_objetivo = d_obj_p, puntos = F)


```


```{r}
cov_prop_p <- diag(c(0.5, 0.5))

cadena_p <- sample_mh_mv(n = 25000, d_objetivo = d_obj_p, cov_propuesta = cov_prop_p, p_inicial = c(0,0))

cadena_p$probabilidad_aceptacion


acf(cadena_p$muestra_mh[-1]) # Decorar

plot_trace(cadena_p$muestra_mh)

plot_hotmap(cadena_p$muestra_mh[-1], d_objetivo = d_obj_p, puntos = F)

```


```{r}
# i
mean((cadena_p$muestra_mh$dim_1 > 0 & cadena_p$muestra_mh$dim_1 < 1) & 
       (cadena_p$muestra_mh$dim_2 > 0 & cadena_p$muestra_mh$dim_2 < 1))

# ii
mean((cadena_p$muestra_mh$dim_1 > -1 & cadena_p$muestra_mh$dim_1 < 0) & 
       (cadena_p$muestra_mh$dim_2 > 0 & cadena_p$muestra_mh$dim_2 < 1))

# iii
mean((cadena_p$muestra_mh$dim_1 > 1 & cadena_p$muestra_mh$dim_1 < 2) & 
       (cadena_p$muestra_mh$dim_2 > 2 & cadena_p$muestra_mh$dim_2 < 3))
```

```{r}
d_integral <- function(x, y) exp(-((0.5-x)^2 + 5 *(y-x^2)^2))

integral2(d_integral, xmin = 0, xmax = 1, ymin = 0, ymax = 1)$Q

integral2(d_integral, xmin = -1, xmax = 0, ymin = 0, ymax = 1)$Q

integral2(d_integral, xmin = 1, xmax = 2, ymin = 2, ymax = 3)$Q

```


# Preguntas y propuestas

Agregar n_eff en mh univarido
Preguntar por la probabilidad de salto en mh_multivariado
comparar las probabilidades tmb por monte carlo